{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T19:41:46.700168Z",
     "start_time": "2024-03-29T19:41:46.684937Z"
    }
   },
   "outputs": [],
   "source": [
    "from EDA_script import *\n",
    "# train_x_raw = pd.read_csv(\"../01-Data/X_train.csv\", low_memory = True, index_col=0)\n",
    "# train_y_raw = pd.read_csv(\"../01-Data/y_train.csv\", low_memory = True, index_col=0)\n",
    "# test_x_raw = pd.read_csv(\"../01-Data/X_test.csv\", low_memory=True, index_col=0)\n",
    "\n",
    "# df_train = pd.DataFrame(train_x_raw)\n",
    "# df_test = pd.DataFrame(test_x_raw)\n",
    "# df_y = pd.DataFrame(train_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646daeb6719b84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Model Run\n",
    "##  xgBoost model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b65d6aaf6aa41b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T19:41:46.716168Z",
     "start_time": "2024-03-29T19:41:46.705093Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Remap labels\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d5d02b4bf4f48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51e53c89851a293",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T19:48:21.426640Z",
     "start_time": "2024-03-29T19:41:46.721313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m-0.8701  \u001B[0m | \u001B[0m0.5247   \u001B[0m | \u001B[0m0.2857   \u001B[0m | \u001B[0m0.732    \u001B[0m | \u001B[0m7.191    \u001B[0m | \u001B[0m1.78     \u001B[0m | \u001B[0m0.156    \u001B[0m | \u001B[0m1.232    \u001B[0m | \u001B[0m0.8197   \u001B[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 56\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Perform Bayesian Optimization\u001B[39;00m\n\u001B[0;32m     55\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m BayesianOptimization(f\u001B[38;5;241m=\u001B[39mxgb_cv_score, pbounds\u001B[38;5;241m=\u001B[39mpbounds, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m(\u001B[49m\u001B[43minit_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py:310\u001B[0m, in \u001B[0;36mBayesianOptimization.maximize\u001B[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001B[0m\n\u001B[0;32m    308\u001B[0m     x_probe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuggest(util)\n\u001B[0;32m    309\u001B[0m     iteration \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprobe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_probe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlazy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bounds_transformer \u001B[38;5;129;01mand\u001B[39;00m iteration \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;66;03m# The bounds transformer should only modify the bounds after\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;66;03m# the init_points points (only for the true iterations)\u001B[39;00m\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_bounds(\n\u001B[0;32m    316\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bounds_transformer\u001B[38;5;241m.\u001B[39mtransform(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_space))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py:208\u001B[0m, in \u001B[0;36mBayesianOptimization.probe\u001B[1;34m(self, params, lazy)\u001B[0m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_queue\u001B[38;5;241m.\u001B[39madd(params)\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 208\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprobe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch(Events\u001B[38;5;241m.\u001B[39mOPTIMIZATION_STEP)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bayes_opt\\target_space.py:236\u001B[0m, in \u001B[0;36mTargetSpace.probe\u001B[1;34m(self, params)\u001B[0m\n\u001B[0;32m    234\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_as_array(params)\n\u001B[0;32m    235\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_keys, x))\n\u001B[1;32m--> 236\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constraint \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister(x, target)\n",
      "Cell \u001B[1;32mIn[15], line 31\u001B[0m, in \u001B[0;36mxgb_cv_score\u001B[1;34m(max_depth, gamma, colsample_bytree, subsample, eta, reg_lambda, reg_alpha, min_child_weight)\u001B[0m\n\u001B[0;32m     28\u001B[0m watchlist \u001B[38;5;241m=\u001B[39m [(xgb_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m), (xgb_valid, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Add early_stopping_rounds\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mxgb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxgb_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_boost_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwatchlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Predict using the best iteration\u001B[39;00m\n\u001B[0;32m     34\u001B[0m preds \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(xgb_valid)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:730\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[0;32m    729\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[1;32m--> 730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:181\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 181\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:2051\u001B[0m, in \u001B[0;36mBooster.update\u001B[1;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[0;32m   2047\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dmatrix_features(dtrain)\n\u001B[0;32m   2049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2050\u001B[0m     _check_call(\n\u001B[1;32m-> 2051\u001B[0m         \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2052\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\n\u001B[0;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2054\u001B[0m     )\n\u001B[0;32m   2055\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2056\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def xgb_cv_score(max_depth, gamma, colsample_bytree, subsample, eta, reg_lambda, reg_alpha, min_child_weight):\n",
    "    \"\"\"\n",
    "    Computes the cross-validated log loss for given hyperparameter settings using Stratified K-Fold.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'subsample': subsample,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'lambda': reg_lambda,\n",
    "        'alpha': reg_alpha,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    log_loss_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(df_train, df_y):\n",
    "        xgb_train = xgb.DMatrix(df_train.iloc[train_index], label=df_y.iloc[train_index])\n",
    "        xgb_valid = xgb.DMatrix(df_train.iloc[test_index], label=df_y.iloc[test_index])\n",
    "        \n",
    "        watchlist = [(xgb_train, 'train'), (xgb_valid, 'eval')]\n",
    "\n",
    "        # Add early_stopping_rounds\n",
    "        model = xgb.train(params, xgb_train, num_boost_round=1000, evals=watchlist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # Predict using the best iteration\n",
    "        preds = model.predict(xgb_valid)\n",
    "        #preds = model.predict(xgb_valid, ntree_limit=(model.best_iteration + 1) * params['num_class']\n",
    "\n",
    "        log_loss_score = log_loss(df_y.iloc[test_index], preds, labels=list(range(5)))\n",
    "        log_loss_scores.append(log_loss_score)\n",
    "\n",
    "    return -np.mean(log_loss_scores)\n",
    "\n",
    "# Define the hyperparameter bounds\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 0.9),\n",
    "    'subsample': (0.3, 0.9),\n",
    "    'eta': (0.01, 0.3),\n",
    "    'reg_lambda': (1, 5),\n",
    "    'reg_alpha': (0, 1),\n",
    "    'min_child_weight': (1, 6),\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=xgb_cv_score, pbounds=pbounds, random_state=42, verbose=2)\n",
    "optimizer.maximize(init_points=10, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ffd766a3e1d0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CV xgboost train with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10d0df175935aa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-29T19:48:21.429640Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Best parameters from optimization\n",
    "best_params = {\n",
    "    'max_depth': int(optimizer.max['params']['max_depth']),\n",
    "    'gamma': optimizer.max['params']['gamma'],\n",
    "    'colsample_bytree': optimizer.max['params']['colsample_bytree'],\n",
    "    'subsample': optimizer.max['params']['subsample'],\n",
    "    'eta': optimizer.max['params']['eta'],\n",
    "    'lambda': optimizer.max['params']['reg_lambda'],\n",
    "    'alpha': optimizer.max['params']['reg_alpha'],\n",
    "    'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(best_params, dtrain, num_boost_round=1000, evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=10, evals_result=evals_result, verbose_eval=True)\n",
    "\n",
    "# Evaluate and print the final training and validation loss\n",
    "train_last_eval = evals_result['train']['mlogloss'][-1]\n",
    "val_last_eval = evals_result['val']['mlogloss'][-1]\n",
    "\n",
    "print(f\"Training Multiclass Logarithmic Loss: {train_last_eval}\")\n",
    "print(f\"Validation Multiclass Logarithmic Loss: {val_last_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527521c5801211",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate Submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6668498e2687f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-29T19:48:21.430640Z"
    }
   },
   "outputs": [],
   "source": [
    "# submission_df = pd.DataFrame(y_test_probs, columns=class_mapping.values())\n",
    "# submission_df.columns = ['no answer', 'very important', 'quite important', 'not important', 'not at all important']\n",
    "# submission_df.insert(0, 'id', df_test.index)\n",
    "# \n",
    "# # Save the submission file\n",
    "# submission_file = ('submission.csv')\n",
    "# submission_df.to_csv(submission_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
