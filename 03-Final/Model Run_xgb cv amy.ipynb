{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T19:41:46.700168Z",
     "start_time": "2024-03-29T19:41:46.684937Z"
    }
   },
   "outputs": [],
   "source": [
    "from EDA_script import *\n",
    "# train_x_raw = pd.read_csv(\"../01-Data/X_train.csv\", low_memory = True, index_col=0)\n",
    "# train_y_raw = pd.read_csv(\"../01-Data/y_train.csv\", low_memory = True, index_col=0)\n",
    "# test_x_raw = pd.read_csv(\"../01-Data/X_test.csv\", low_memory=True, index_col=0)\n",
    "\n",
    "# df_train = pd.DataFrame(train_x_raw)\n",
    "# df_test = pd.DataFrame(test_x_raw)\n",
    "# df_y = pd.DataFrame(train_y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e43b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add to EDA\n",
    "for column in set(df_train.columns) - set(df_test.columns):\n",
    "    df_test[column] = 0\n",
    "\n",
    "df_test = df_test[df_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646daeb6719b84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Model Run\n",
    "##  xgBoost model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b65d6aaf6aa41b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T19:41:46.716168Z",
     "start_time": "2024-03-29T19:41:46.705093Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Remap labels\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d5d02b4bf4f48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51e53c89851a293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T19:48:21.426640Z",
     "start_time": "2024-03-29T19:41:46.721313Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.8706  \u001b[0m | \u001b[0m0.5247   \u001b[0m | \u001b[0m0.2857   \u001b[0m | \u001b[0m0.732    \u001b[0m | \u001b[0m7.191    \u001b[0m | \u001b[0m1.78     \u001b[0m | \u001b[0m0.156    \u001b[0m | \u001b[0m1.232    \u001b[0m | \u001b[0m0.8197   \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-0.888   \u001b[0m | \u001b[0m0.6607   \u001b[0m | \u001b[0m0.2153   \u001b[0m | \u001b[0m0.02058  \u001b[0m | \u001b[0m9.789    \u001b[0m | \u001b[0m5.162    \u001b[0m | \u001b[0m0.2123   \u001b[0m | \u001b[0m1.727    \u001b[0m | \u001b[0m0.41     \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-0.8598  \u001b[0m | \u001b[95m0.4825   \u001b[0m | \u001b[95m0.1622   \u001b[0m | \u001b[95m0.4319   \u001b[0m | \u001b[95m5.039    \u001b[0m | \u001b[95m4.059    \u001b[0m | \u001b[95m0.1395   \u001b[0m | \u001b[95m2.169    \u001b[0m | \u001b[95m0.5198   \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.876   \u001b[0m | \u001b[0m0.5736   \u001b[0m | \u001b[0m0.2377   \u001b[0m | \u001b[0m0.1997   \u001b[0m | \u001b[0m6.6      \u001b[0m | \u001b[0m3.962    \u001b[0m | \u001b[0m0.04645  \u001b[0m | \u001b[0m3.43     \u001b[0m | \u001b[0m0.4023   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.8812  \u001b[0m | \u001b[0m0.339    \u001b[0m | \u001b[0m0.2852   \u001b[0m | \u001b[0m0.9656   \u001b[0m | \u001b[0m8.659    \u001b[0m | \u001b[0m2.523    \u001b[0m | \u001b[0m0.09767  \u001b[0m | \u001b[0m3.737    \u001b[0m | \u001b[0m0.5641   \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.8628  \u001b[0m | \u001b[0m0.3732   \u001b[0m | \u001b[0m0.1536   \u001b[0m | \u001b[0m0.03439  \u001b[0m | \u001b[0m9.365    \u001b[0m | \u001b[0m2.294    \u001b[0m | \u001b[0m0.6625   \u001b[0m | \u001b[0m2.247    \u001b[0m | \u001b[0m0.612    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m-0.8438  \u001b[0m | \u001b[95m0.628    \u001b[0m | \u001b[95m0.06361  \u001b[0m | \u001b[95m0.9696   \u001b[0m | \u001b[95m8.426    \u001b[0m | \u001b[95m5.697    \u001b[0m | \u001b[95m0.8948   \u001b[0m | \u001b[95m3.392    \u001b[0m | \u001b[95m0.8531   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.8519  \u001b[0m | \u001b[0m0.3531   \u001b[0m | \u001b[0m0.06684  \u001b[0m | \u001b[0m0.04523  \u001b[0m | \u001b[0m5.277    \u001b[0m | \u001b[0m2.943    \u001b[0m | \u001b[0m0.2713   \u001b[0m | \u001b[0m4.315    \u001b[0m | \u001b[0m0.5141   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.8699  \u001b[0m | \u001b[0m0.4686   \u001b[0m | \u001b[0m0.1674   \u001b[0m | \u001b[0m0.1409   \u001b[0m | \u001b[0m8.615    \u001b[0m | \u001b[0m1.373    \u001b[0m | \u001b[0m0.9869   \u001b[0m | \u001b[0m4.089    \u001b[0m | \u001b[0m0.4192   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.8923  \u001b[0m | \u001b[0m0.3033   \u001b[0m | \u001b[0m0.2465   \u001b[0m | \u001b[0m0.7069   \u001b[0m | \u001b[0m8.103    \u001b[0m | \u001b[0m4.856    \u001b[0m | \u001b[0m0.07404  \u001b[0m | \u001b[0m2.434    \u001b[0m | \u001b[0m0.3695   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.8627  \u001b[0m | \u001b[0m0.5653   \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.3403   \u001b[0m | \u001b[0m5.258    \u001b[0m | \u001b[0m3.122    \u001b[0m | \u001b[0m0.1456   \u001b[0m | \u001b[0m4.325    \u001b[0m | \u001b[0m0.5092   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.8804  \u001b[0m | \u001b[0m0.7283   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m8.509    \u001b[0m | \u001b[0m5.962    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.701    \u001b[0m | \u001b[0m0.9      \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.8606  \u001b[0m | \u001b[0m0.3818   \u001b[0m | \u001b[0m0.2054   \u001b[0m | \u001b[0m0.5112   \u001b[0m | \u001b[0m3.184    \u001b[0m | \u001b[0m1.278    \u001b[0m | \u001b[0m0.248    \u001b[0m | \u001b[0m4.885    \u001b[0m | \u001b[0m0.7738   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.8684  \u001b[0m | \u001b[0m0.6289   \u001b[0m | \u001b[0m0.2746   \u001b[0m | \u001b[0m0.08454  \u001b[0m | \u001b[0m6.145    \u001b[0m | \u001b[0m1.993    \u001b[0m | \u001b[0m0.5907   \u001b[0m | \u001b[0m2.554    \u001b[0m | \u001b[0m0.6737   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.8602  \u001b[0m | \u001b[0m0.4569   \u001b[0m | \u001b[0m0.2478   \u001b[0m | \u001b[0m0.9794   \u001b[0m | \u001b[0m4.059    \u001b[0m | \u001b[0m5.217    \u001b[0m | \u001b[0m0.7475   \u001b[0m | \u001b[0m3.607    \u001b[0m | \u001b[0m0.844    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8579  \u001b[0m | \u001b[0m0.6821   \u001b[0m | \u001b[0m0.1396   \u001b[0m | \u001b[0m0.6675   \u001b[0m | \u001b[0m7.925    \u001b[0m | \u001b[0m5.272    \u001b[0m | \u001b[0m0.6405   \u001b[0m | \u001b[0m3.958    \u001b[0m | \u001b[0m0.5861   \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.8623  \u001b[0m | \u001b[0m0.7033   \u001b[0m | \u001b[0m0.2349   \u001b[0m | \u001b[0m0.1625   \u001b[0m | \u001b[0m3.904    \u001b[0m | \u001b[0m2.725    \u001b[0m | \u001b[0m0.02849  \u001b[0m | \u001b[0m1.476    \u001b[0m | \u001b[0m0.8702   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.8705  \u001b[0m | \u001b[0m0.7548   \u001b[0m | \u001b[0m0.2907   \u001b[0m | \u001b[0m0.97     \u001b[0m | \u001b[0m8.174    \u001b[0m | \u001b[0m5.02     \u001b[0m | \u001b[0m0.7308   \u001b[0m | \u001b[0m3.707    \u001b[0m | \u001b[0m0.843    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.8632  \u001b[0m | \u001b[0m0.8169   \u001b[0m | \u001b[0m0.2055   \u001b[0m | \u001b[0m0.07548  \u001b[0m | \u001b[0m6.628    \u001b[0m | \u001b[0m2.352    \u001b[0m | \u001b[0m0.6758   \u001b[0m | \u001b[0m2.938    \u001b[0m | \u001b[0m0.6722   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.8533  \u001b[0m | \u001b[0m0.3811   \u001b[0m | \u001b[0m0.01815  \u001b[0m | \u001b[0m0.4813   \u001b[0m | \u001b[0m8.071    \u001b[0m | \u001b[0m5.515    \u001b[0m | \u001b[0m0.1501   \u001b[0m | \u001b[0m4.31     \u001b[0m | \u001b[0m0.6962   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.8626  \u001b[0m | \u001b[0m0.5501   \u001b[0m | \u001b[0m0.1699   \u001b[0m | \u001b[0m0.961    \u001b[0m | \u001b[0m7.87     \u001b[0m | \u001b[0m5.572    \u001b[0m | \u001b[0m0.0008004\u001b[0m | \u001b[0m4.915    \u001b[0m | \u001b[0m0.4799   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.8455  \u001b[0m | \u001b[0m0.7029   \u001b[0m | \u001b[0m0.03909  \u001b[0m | \u001b[0m0.3938   \u001b[0m | \u001b[0m7.641    \u001b[0m | \u001b[0m4.89     \u001b[0m | \u001b[0m0.5738   \u001b[0m | \u001b[0m1.981    \u001b[0m | \u001b[0m0.8681   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.8508  \u001b[0m | \u001b[0m0.8754   \u001b[0m | \u001b[0m0.02484  \u001b[0m | \u001b[0m0.6285   \u001b[0m | \u001b[0m8.947    \u001b[0m | \u001b[0m1.317    \u001b[0m | \u001b[0m0.4333   \u001b[0m | \u001b[0m1.005    \u001b[0m | \u001b[0m0.3311   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.8577  \u001b[0m | \u001b[0m0.7718   \u001b[0m | \u001b[0m0.0498   \u001b[0m | \u001b[0m0.9352   \u001b[0m | \u001b[0m5.249    \u001b[0m | \u001b[0m3.988    \u001b[0m | \u001b[0m0.9345   \u001b[0m | \u001b[0m1.902    \u001b[0m | \u001b[0m0.3138   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.8504  \u001b[0m | \u001b[0m0.7151   \u001b[0m | \u001b[0m0.06942  \u001b[0m | \u001b[0m0.7271   \u001b[0m | \u001b[0m9.056    \u001b[0m | \u001b[0m1.351    \u001b[0m | \u001b[0m0.09618  \u001b[0m | \u001b[0m1.752    \u001b[0m | \u001b[0m0.66     \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.8568  \u001b[0m | \u001b[0m0.6496   \u001b[0m | \u001b[0m0.0286   \u001b[0m | \u001b[0m0.09428  \u001b[0m | \u001b[0m5.617    \u001b[0m | \u001b[0m4.413    \u001b[0m | \u001b[0m0.4914   \u001b[0m | \u001b[0m2.852    \u001b[0m | \u001b[0m0.7551   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.8595  \u001b[0m | \u001b[0m0.7344   \u001b[0m | \u001b[0m0.07754  \u001b[0m | \u001b[0m0.1373   \u001b[0m | \u001b[0m3.957    \u001b[0m | \u001b[0m4.887    \u001b[0m | \u001b[0m0.6454   \u001b[0m | \u001b[0m1.971    \u001b[0m | \u001b[0m0.6032   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.8523  \u001b[0m | \u001b[0m0.7469   \u001b[0m | \u001b[0m0.08485  \u001b[0m | \u001b[0m0.2328   \u001b[0m | \u001b[0m9.616    \u001b[0m | \u001b[0m1.114    \u001b[0m | \u001b[0m0.5828   \u001b[0m | \u001b[0m3.591    \u001b[0m | \u001b[0m0.832    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.851   \u001b[0m | \u001b[0m0.551    \u001b[0m | \u001b[0m0.1116   \u001b[0m | \u001b[0m0.5531   \u001b[0m | \u001b[0m6.107    \u001b[0m | \u001b[0m4.071    \u001b[0m | \u001b[0m0.1182   \u001b[0m | \u001b[0m2.282    \u001b[0m | \u001b[0m0.8009   \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.8599  \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.1236   \u001b[0m | \u001b[0m0.8248   \u001b[0m | \u001b[0m7.937    \u001b[0m | \u001b[0m1.381    \u001b[0m | \u001b[0m0.9489   \u001b[0m | \u001b[0m3.694    \u001b[0m | \u001b[0m0.5076   \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def xgb_cv_score(max_depth, gamma, colsample_bytree, subsample, eta, reg_lambda, reg_alpha, min_child_weight):\n",
    "    \"\"\"\n",
    "    Computes the cross-validated log loss for given hyperparameter settings using Stratified K-Fold.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'device': 'cuda',\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'subsample': subsample,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'lambda': reg_lambda,\n",
    "        'alpha': reg_alpha,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    log_loss_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(df_train, df_y):\n",
    "        xgb_train = xgb.DMatrix(df_train.iloc[train_index], label=df_y.iloc[train_index])\n",
    "        xgb_valid = xgb.DMatrix(df_train.iloc[test_index], label=df_y.iloc[test_index])\n",
    "        \n",
    "        watchlist = [(xgb_train, 'train'), (xgb_valid, 'eval')]\n",
    "\n",
    "        # Add early_stopping_rounds\n",
    "        model = xgb.train(params, xgb_train, num_boost_round=500, evals=watchlist, early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "        # Predict using the best iteration\n",
    "        preds = model.predict(xgb_valid)\n",
    "        #preds = model.predict(xgb_valid, ntree_limit=(model.best_iteration + 1) * params['num_class']\n",
    "\n",
    "        log_loss_score = log_loss(df_y.iloc[test_index], preds, labels=list(range(5)))\n",
    "        log_loss_scores.append(log_loss_score)\n",
    "\n",
    "    return -np.mean(log_loss_scores)\n",
    "\n",
    "# Define the hyperparameter bounds\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 0.9),\n",
    "    'subsample': (0.3, 0.9),\n",
    "    'eta': (0.01, 0.3),\n",
    "    'reg_lambda': (1, 5),\n",
    "    'reg_alpha': (0, 1),\n",
    "    'min_child_weight': (1, 6),\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=xgb_cv_score, pbounds=pbounds, random_state=42, verbose=2)\n",
    "optimizer.maximize(init_points=10, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ffd766a3e1d0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CV xgboost train with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e10d0df175935aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-29T19:48:21.429640Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Best parameters from optimization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m best_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mmax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[1;32m---> 20\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m dval \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_val, label\u001b[38;5;241m=\u001b[39my_val)\n\u001b[0;32m     23\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:857\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_split_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_split_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:1089\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[1;32m-> 1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data) \u001b[38;5;129;01mor\u001b[39;00m _is_cudf_ser(data):\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_cudf_df(\n\u001b[0;32m   1094\u001b[0m         data, missing, threads, feature_names, feature_types, enable_categorical\n\u001b[0;32m   1095\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:522\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[1;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pandas_df\u001b[39m(\n\u001b[0;32m    515\u001b[0m     data: DataFrame,\n\u001b[0;32m    516\u001b[0m     enable_categorical: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    521\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[1;32m--> 522\u001b[0m     data, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:508\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[1;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot have multiple columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m dtype \u001b[38;5;241m=\u001b[39m meta_type \u001b[38;5;28;01mif\u001b[39;00m meta_type \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m--> 508\u001b[0m arr: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_type:\n\u001b[0;32m    510\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:12651\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12577\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12579\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12580\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12581\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12649\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12650\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1692\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1692\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1733\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m   1732\u001b[0m     rl \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mmgr_locs\n\u001b[1;32m-> 1733\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1734\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1735\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Best parameters from optimization\n",
    "best_params = {\n",
    "    'max_depth': int(optimizer.max['params']['max_depth']),\n",
    "    'gamma': optimizer.max['params']['gamma'],\n",
    "    'colsample_bytree': optimizer.max['params']['colsample_bytree'],\n",
    "    'subsample': optimizer.max['params']['subsample'],\n",
    "    'eta': optimizer.max['params']['eta'],\n",
    "    'lambda': optimizer.max['params']['reg_lambda'],\n",
    "    'alpha': optimizer.max['params']['reg_alpha'],\n",
    "    'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(best_params, dtrain, num_boost_round=1000, evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=10, evals_result=evals_result, verbose_eval=True)\n",
    "\n",
    "# Evaluate and print the final training and validation loss\n",
    "train_last_eval = evals_result['train']['mlogloss'][-1]\n",
    "val_last_eval = evals_result['val']['mlogloss'][-1]\n",
    "\n",
    "print(f\"Training Multiclass Logarithmic Loss: {train_last_eval}\")\n",
    "print(f\"Validation Multiclass Logarithmic Loss: {val_last_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527521c5801211",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate Submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "938dc9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6280261676059677,\n",
       " 'eta': 0.06360779210240283,\n",
       " 'gamma': 0.9695846277645586,\n",
       " 'max_depth': 8.425929763527801,\n",
       " 'min_child_weight': 5.697494707820946,\n",
       " 'reg_alpha': 0.8948273504276488,\n",
       " 'reg_lambda': 3.3915999152443406,\n",
       " 'subsample': 0.8531245410138701}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7033bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.57397\tval-mlogloss:1.57570\n",
      "[1]\ttrain-mlogloss:1.53936\tval-mlogloss:1.54304\n",
      "[2]\ttrain-mlogloss:1.50420\tval-mlogloss:1.50996\n",
      "[3]\ttrain-mlogloss:1.47389\tval-mlogloss:1.48096\n",
      "[4]\ttrain-mlogloss:1.44335\tval-mlogloss:1.45206\n",
      "[5]\ttrain-mlogloss:1.41496\tval-mlogloss:1.42526\n",
      "[6]\ttrain-mlogloss:1.38946\tval-mlogloss:1.40130\n",
      "[7]\ttrain-mlogloss:1.36431\tval-mlogloss:1.37772\n",
      "[8]\ttrain-mlogloss:1.33988\tval-mlogloss:1.35509\n",
      "[9]\ttrain-mlogloss:1.31778\tval-mlogloss:1.33462\n",
      "[10]\ttrain-mlogloss:1.29665\tval-mlogloss:1.31510\n",
      "[11]\ttrain-mlogloss:1.27684\tval-mlogloss:1.29695\n",
      "[12]\ttrain-mlogloss:1.25763\tval-mlogloss:1.27888\n",
      "[13]\ttrain-mlogloss:1.24071\tval-mlogloss:1.26295\n",
      "[14]\ttrain-mlogloss:1.22339\tval-mlogloss:1.24713\n",
      "[15]\ttrain-mlogloss:1.20807\tval-mlogloss:1.23296\n",
      "[16]\ttrain-mlogloss:1.19401\tval-mlogloss:1.22009\n",
      "[17]\ttrain-mlogloss:1.18004\tval-mlogloss:1.20713\n",
      "[18]\ttrain-mlogloss:1.16659\tval-mlogloss:1.19457\n",
      "[19]\ttrain-mlogloss:1.15283\tval-mlogloss:1.18187\n",
      "[20]\ttrain-mlogloss:1.14044\tval-mlogloss:1.17077\n",
      "[21]\ttrain-mlogloss:1.12861\tval-mlogloss:1.16003\n",
      "[22]\ttrain-mlogloss:1.11790\tval-mlogloss:1.15021\n",
      "[23]\ttrain-mlogloss:1.10667\tval-mlogloss:1.14004\n",
      "[24]\ttrain-mlogloss:1.09566\tval-mlogloss:1.13034\n",
      "[25]\ttrain-mlogloss:1.08526\tval-mlogloss:1.12097\n",
      "[26]\ttrain-mlogloss:1.07483\tval-mlogloss:1.11165\n",
      "[27]\ttrain-mlogloss:1.06508\tval-mlogloss:1.10291\n",
      "[28]\ttrain-mlogloss:1.05580\tval-mlogloss:1.09459\n",
      "[29]\ttrain-mlogloss:1.04733\tval-mlogloss:1.08693\n",
      "[30]\ttrain-mlogloss:1.03852\tval-mlogloss:1.07899\n",
      "[31]\ttrain-mlogloss:1.03028\tval-mlogloss:1.07184\n",
      "[32]\ttrain-mlogloss:1.02282\tval-mlogloss:1.06517\n",
      "[33]\ttrain-mlogloss:1.01535\tval-mlogloss:1.05891\n",
      "[34]\ttrain-mlogloss:1.00827\tval-mlogloss:1.05262\n",
      "[35]\ttrain-mlogloss:1.00146\tval-mlogloss:1.04653\n",
      "[36]\ttrain-mlogloss:0.99491\tval-mlogloss:1.04089\n",
      "[37]\ttrain-mlogloss:0.98850\tval-mlogloss:1.03527\n",
      "[38]\ttrain-mlogloss:0.98197\tval-mlogloss:1.02955\n",
      "[39]\ttrain-mlogloss:0.97599\tval-mlogloss:1.02424\n",
      "[40]\ttrain-mlogloss:0.97013\tval-mlogloss:1.01920\n",
      "[41]\ttrain-mlogloss:0.96423\tval-mlogloss:1.01407\n",
      "[42]\ttrain-mlogloss:0.95861\tval-mlogloss:1.00925\n",
      "[43]\ttrain-mlogloss:0.95363\tval-mlogloss:1.00495\n",
      "[44]\ttrain-mlogloss:0.94845\tval-mlogloss:1.00058\n",
      "[45]\ttrain-mlogloss:0.94339\tval-mlogloss:0.99637\n",
      "[46]\ttrain-mlogloss:0.93879\tval-mlogloss:0.99262\n",
      "[47]\ttrain-mlogloss:0.93427\tval-mlogloss:0.98894\n",
      "[48]\ttrain-mlogloss:0.92987\tval-mlogloss:0.98529\n",
      "[49]\ttrain-mlogloss:0.92562\tval-mlogloss:0.98187\n",
      "[50]\ttrain-mlogloss:0.92169\tval-mlogloss:0.97870\n",
      "[51]\ttrain-mlogloss:0.91776\tval-mlogloss:0.97548\n",
      "[52]\ttrain-mlogloss:0.91380\tval-mlogloss:0.97253\n",
      "[53]\ttrain-mlogloss:0.91001\tval-mlogloss:0.96950\n",
      "[54]\ttrain-mlogloss:0.90621\tval-mlogloss:0.96659\n",
      "[55]\ttrain-mlogloss:0.90262\tval-mlogloss:0.96385\n",
      "[56]\ttrain-mlogloss:0.89941\tval-mlogloss:0.96130\n",
      "[57]\ttrain-mlogloss:0.89594\tval-mlogloss:0.95873\n",
      "[58]\ttrain-mlogloss:0.89272\tval-mlogloss:0.95631\n",
      "[59]\ttrain-mlogloss:0.88983\tval-mlogloss:0.95388\n",
      "[60]\ttrain-mlogloss:0.88664\tval-mlogloss:0.95126\n",
      "[61]\ttrain-mlogloss:0.88360\tval-mlogloss:0.94884\n",
      "[62]\ttrain-mlogloss:0.88065\tval-mlogloss:0.94657\n",
      "[63]\ttrain-mlogloss:0.87781\tval-mlogloss:0.94437\n",
      "[64]\ttrain-mlogloss:0.87498\tval-mlogloss:0.94226\n",
      "[65]\ttrain-mlogloss:0.87240\tval-mlogloss:0.94012\n",
      "[66]\ttrain-mlogloss:0.86958\tval-mlogloss:0.93824\n",
      "[67]\ttrain-mlogloss:0.86728\tval-mlogloss:0.93644\n",
      "[68]\ttrain-mlogloss:0.86494\tval-mlogloss:0.93476\n",
      "[69]\ttrain-mlogloss:0.86269\tval-mlogloss:0.93305\n",
      "[70]\ttrain-mlogloss:0.86032\tval-mlogloss:0.93121\n",
      "[71]\ttrain-mlogloss:0.85783\tval-mlogloss:0.92937\n",
      "[72]\ttrain-mlogloss:0.85562\tval-mlogloss:0.92778\n",
      "[73]\ttrain-mlogloss:0.85328\tval-mlogloss:0.92608\n",
      "[74]\ttrain-mlogloss:0.85132\tval-mlogloss:0.92464\n",
      "[75]\ttrain-mlogloss:0.84937\tval-mlogloss:0.92322\n",
      "[76]\ttrain-mlogloss:0.84742\tval-mlogloss:0.92175\n",
      "[77]\ttrain-mlogloss:0.84532\tval-mlogloss:0.92022\n",
      "[78]\ttrain-mlogloss:0.84332\tval-mlogloss:0.91877\n",
      "[79]\ttrain-mlogloss:0.84128\tval-mlogloss:0.91739\n",
      "[80]\ttrain-mlogloss:0.83920\tval-mlogloss:0.91591\n",
      "[81]\ttrain-mlogloss:0.83742\tval-mlogloss:0.91460\n",
      "[82]\ttrain-mlogloss:0.83564\tval-mlogloss:0.91341\n",
      "[83]\ttrain-mlogloss:0.83366\tval-mlogloss:0.91198\n",
      "[84]\ttrain-mlogloss:0.83183\tval-mlogloss:0.91078\n",
      "[85]\ttrain-mlogloss:0.83021\tval-mlogloss:0.90969\n",
      "[86]\ttrain-mlogloss:0.82860\tval-mlogloss:0.90859\n",
      "[87]\ttrain-mlogloss:0.82693\tval-mlogloss:0.90759\n",
      "[88]\ttrain-mlogloss:0.82533\tval-mlogloss:0.90662\n",
      "[89]\ttrain-mlogloss:0.82388\tval-mlogloss:0.90563\n",
      "[90]\ttrain-mlogloss:0.82237\tval-mlogloss:0.90466\n",
      "[91]\ttrain-mlogloss:0.82080\tval-mlogloss:0.90366\n",
      "[92]\ttrain-mlogloss:0.81943\tval-mlogloss:0.90271\n",
      "[93]\ttrain-mlogloss:0.81791\tval-mlogloss:0.90176\n",
      "[94]\ttrain-mlogloss:0.81653\tval-mlogloss:0.90078\n",
      "[95]\ttrain-mlogloss:0.81499\tval-mlogloss:0.89988\n",
      "[96]\ttrain-mlogloss:0.81348\tval-mlogloss:0.89903\n",
      "[97]\ttrain-mlogloss:0.81201\tval-mlogloss:0.89815\n",
      "[98]\ttrain-mlogloss:0.81074\tval-mlogloss:0.89739\n",
      "[99]\ttrain-mlogloss:0.80932\tval-mlogloss:0.89662\n",
      "[100]\ttrain-mlogloss:0.80800\tval-mlogloss:0.89578\n",
      "[101]\ttrain-mlogloss:0.80663\tval-mlogloss:0.89507\n",
      "[102]\ttrain-mlogloss:0.80539\tval-mlogloss:0.89422\n",
      "[103]\ttrain-mlogloss:0.80426\tval-mlogloss:0.89358\n",
      "[104]\ttrain-mlogloss:0.80316\tval-mlogloss:0.89299\n",
      "[105]\ttrain-mlogloss:0.80220\tval-mlogloss:0.89238\n",
      "[106]\ttrain-mlogloss:0.80103\tval-mlogloss:0.89175\n",
      "[107]\ttrain-mlogloss:0.79989\tval-mlogloss:0.89106\n",
      "[108]\ttrain-mlogloss:0.79886\tval-mlogloss:0.89051\n",
      "[109]\ttrain-mlogloss:0.79780\tval-mlogloss:0.88991\n",
      "[110]\ttrain-mlogloss:0.79675\tval-mlogloss:0.88926\n",
      "[111]\ttrain-mlogloss:0.79555\tval-mlogloss:0.88860\n",
      "[112]\ttrain-mlogloss:0.79463\tval-mlogloss:0.88808\n",
      "[113]\ttrain-mlogloss:0.79353\tval-mlogloss:0.88750\n",
      "[114]\ttrain-mlogloss:0.79250\tval-mlogloss:0.88698\n",
      "[115]\ttrain-mlogloss:0.79148\tval-mlogloss:0.88636\n",
      "[116]\ttrain-mlogloss:0.79058\tval-mlogloss:0.88584\n",
      "[117]\ttrain-mlogloss:0.78947\tval-mlogloss:0.88522\n",
      "[118]\ttrain-mlogloss:0.78864\tval-mlogloss:0.88475\n",
      "[119]\ttrain-mlogloss:0.78781\tval-mlogloss:0.88431\n",
      "[120]\ttrain-mlogloss:0.78699\tval-mlogloss:0.88391\n",
      "[121]\ttrain-mlogloss:0.78608\tval-mlogloss:0.88346\n",
      "[122]\ttrain-mlogloss:0.78526\tval-mlogloss:0.88308\n",
      "[123]\ttrain-mlogloss:0.78439\tval-mlogloss:0.88266\n",
      "[124]\ttrain-mlogloss:0.78354\tval-mlogloss:0.88224\n",
      "[125]\ttrain-mlogloss:0.78279\tval-mlogloss:0.88188\n",
      "[126]\ttrain-mlogloss:0.78191\tval-mlogloss:0.88140\n",
      "[127]\ttrain-mlogloss:0.78119\tval-mlogloss:0.88101\n",
      "[128]\ttrain-mlogloss:0.78026\tval-mlogloss:0.88063\n",
      "[129]\ttrain-mlogloss:0.77947\tval-mlogloss:0.88029\n",
      "[130]\ttrain-mlogloss:0.77869\tval-mlogloss:0.87992\n",
      "[131]\ttrain-mlogloss:0.77791\tval-mlogloss:0.87946\n",
      "[132]\ttrain-mlogloss:0.77705\tval-mlogloss:0.87907\n",
      "[133]\ttrain-mlogloss:0.77633\tval-mlogloss:0.87872\n",
      "[134]\ttrain-mlogloss:0.77569\tval-mlogloss:0.87839\n",
      "[135]\ttrain-mlogloss:0.77504\tval-mlogloss:0.87812\n",
      "[136]\ttrain-mlogloss:0.77423\tval-mlogloss:0.87779\n",
      "[137]\ttrain-mlogloss:0.77366\tval-mlogloss:0.87750\n",
      "[138]\ttrain-mlogloss:0.77281\tval-mlogloss:0.87717\n",
      "[139]\ttrain-mlogloss:0.77218\tval-mlogloss:0.87682\n",
      "[140]\ttrain-mlogloss:0.77149\tval-mlogloss:0.87645\n",
      "[141]\ttrain-mlogloss:0.77084\tval-mlogloss:0.87621\n",
      "[142]\ttrain-mlogloss:0.77019\tval-mlogloss:0.87587\n",
      "[143]\ttrain-mlogloss:0.76943\tval-mlogloss:0.87553\n",
      "[144]\ttrain-mlogloss:0.76885\tval-mlogloss:0.87529\n",
      "[145]\ttrain-mlogloss:0.76834\tval-mlogloss:0.87501\n",
      "[146]\ttrain-mlogloss:0.76768\tval-mlogloss:0.87469\n",
      "[147]\ttrain-mlogloss:0.76697\tval-mlogloss:0.87437\n",
      "[148]\ttrain-mlogloss:0.76642\tval-mlogloss:0.87418\n",
      "[149]\ttrain-mlogloss:0.76586\tval-mlogloss:0.87390\n",
      "[150]\ttrain-mlogloss:0.76533\tval-mlogloss:0.87371\n",
      "[151]\ttrain-mlogloss:0.76473\tval-mlogloss:0.87345\n",
      "[152]\ttrain-mlogloss:0.76414\tval-mlogloss:0.87320\n",
      "[153]\ttrain-mlogloss:0.76354\tval-mlogloss:0.87297\n",
      "[154]\ttrain-mlogloss:0.76293\tval-mlogloss:0.87270\n",
      "[155]\ttrain-mlogloss:0.76245\tval-mlogloss:0.87253\n",
      "[156]\ttrain-mlogloss:0.76200\tval-mlogloss:0.87234\n",
      "[157]\ttrain-mlogloss:0.76155\tval-mlogloss:0.87211\n",
      "[158]\ttrain-mlogloss:0.76106\tval-mlogloss:0.87188\n",
      "[159]\ttrain-mlogloss:0.76046\tval-mlogloss:0.87155\n",
      "[160]\ttrain-mlogloss:0.76000\tval-mlogloss:0.87131\n",
      "[161]\ttrain-mlogloss:0.75947\tval-mlogloss:0.87111\n",
      "[162]\ttrain-mlogloss:0.75900\tval-mlogloss:0.87099\n",
      "[163]\ttrain-mlogloss:0.75857\tval-mlogloss:0.87077\n",
      "[164]\ttrain-mlogloss:0.75795\tval-mlogloss:0.87047\n",
      "[165]\ttrain-mlogloss:0.75742\tval-mlogloss:0.87029\n",
      "[166]\ttrain-mlogloss:0.75691\tval-mlogloss:0.87007\n",
      "[167]\ttrain-mlogloss:0.75625\tval-mlogloss:0.86988\n",
      "[168]\ttrain-mlogloss:0.75582\tval-mlogloss:0.86972\n",
      "[169]\ttrain-mlogloss:0.75546\tval-mlogloss:0.86955\n",
      "[170]\ttrain-mlogloss:0.75504\tval-mlogloss:0.86938\n",
      "[171]\ttrain-mlogloss:0.75461\tval-mlogloss:0.86921\n",
      "[172]\ttrain-mlogloss:0.75429\tval-mlogloss:0.86906\n",
      "[173]\ttrain-mlogloss:0.75386\tval-mlogloss:0.86892\n",
      "[174]\ttrain-mlogloss:0.75350\tval-mlogloss:0.86876\n",
      "[175]\ttrain-mlogloss:0.75327\tval-mlogloss:0.86868\n",
      "[176]\ttrain-mlogloss:0.75290\tval-mlogloss:0.86860\n",
      "[177]\ttrain-mlogloss:0.75270\tval-mlogloss:0.86850\n",
      "[178]\ttrain-mlogloss:0.75239\tval-mlogloss:0.86836\n",
      "[179]\ttrain-mlogloss:0.75192\tval-mlogloss:0.86821\n",
      "[180]\ttrain-mlogloss:0.75137\tval-mlogloss:0.86800\n",
      "[181]\ttrain-mlogloss:0.75099\tval-mlogloss:0.86788\n",
      "[182]\ttrain-mlogloss:0.75068\tval-mlogloss:0.86777\n",
      "[183]\ttrain-mlogloss:0.75043\tval-mlogloss:0.86767\n",
      "[184]\ttrain-mlogloss:0.74999\tval-mlogloss:0.86750\n",
      "[185]\ttrain-mlogloss:0.74978\tval-mlogloss:0.86740\n",
      "[186]\ttrain-mlogloss:0.74947\tval-mlogloss:0.86731\n",
      "[187]\ttrain-mlogloss:0.74916\tval-mlogloss:0.86724\n",
      "[188]\ttrain-mlogloss:0.74880\tval-mlogloss:0.86712\n",
      "[189]\ttrain-mlogloss:0.74836\tval-mlogloss:0.86698\n",
      "[190]\ttrain-mlogloss:0.74796\tval-mlogloss:0.86687\n",
      "[191]\ttrain-mlogloss:0.74754\tval-mlogloss:0.86681\n",
      "[192]\ttrain-mlogloss:0.74716\tval-mlogloss:0.86666\n",
      "[193]\ttrain-mlogloss:0.74686\tval-mlogloss:0.86652\n",
      "[194]\ttrain-mlogloss:0.74651\tval-mlogloss:0.86645\n",
      "[195]\ttrain-mlogloss:0.74622\tval-mlogloss:0.86634\n",
      "[196]\ttrain-mlogloss:0.74604\tval-mlogloss:0.86626\n",
      "[197]\ttrain-mlogloss:0.74574\tval-mlogloss:0.86617\n",
      "[198]\ttrain-mlogloss:0.74551\tval-mlogloss:0.86605\n",
      "[199]\ttrain-mlogloss:0.74509\tval-mlogloss:0.86594\n",
      "[200]\ttrain-mlogloss:0.74481\tval-mlogloss:0.86582\n",
      "[201]\ttrain-mlogloss:0.74464\tval-mlogloss:0.86578\n",
      "[202]\ttrain-mlogloss:0.74436\tval-mlogloss:0.86563\n",
      "[203]\ttrain-mlogloss:0.74425\tval-mlogloss:0.86559\n",
      "[204]\ttrain-mlogloss:0.74402\tval-mlogloss:0.86551\n",
      "[205]\ttrain-mlogloss:0.74381\tval-mlogloss:0.86543\n",
      "[206]\ttrain-mlogloss:0.74366\tval-mlogloss:0.86537\n",
      "[207]\ttrain-mlogloss:0.74350\tval-mlogloss:0.86532\n",
      "[208]\ttrain-mlogloss:0.74335\tval-mlogloss:0.86527\n",
      "[209]\ttrain-mlogloss:0.74311\tval-mlogloss:0.86518\n",
      "[210]\ttrain-mlogloss:0.74281\tval-mlogloss:0.86507\n",
      "[211]\ttrain-mlogloss:0.74256\tval-mlogloss:0.86498\n",
      "[212]\ttrain-mlogloss:0.74234\tval-mlogloss:0.86489\n",
      "[213]\ttrain-mlogloss:0.74213\tval-mlogloss:0.86482\n",
      "[214]\ttrain-mlogloss:0.74189\tval-mlogloss:0.86472\n",
      "[215]\ttrain-mlogloss:0.74168\tval-mlogloss:0.86466\n",
      "[216]\ttrain-mlogloss:0.74155\tval-mlogloss:0.86459\n",
      "[217]\ttrain-mlogloss:0.74137\tval-mlogloss:0.86453\n",
      "[218]\ttrain-mlogloss:0.74120\tval-mlogloss:0.86448\n",
      "[219]\ttrain-mlogloss:0.74103\tval-mlogloss:0.86444\n",
      "[220]\ttrain-mlogloss:0.74086\tval-mlogloss:0.86443\n",
      "[221]\ttrain-mlogloss:0.74069\tval-mlogloss:0.86435\n",
      "[222]\ttrain-mlogloss:0.74043\tval-mlogloss:0.86428\n",
      "[223]\ttrain-mlogloss:0.74014\tval-mlogloss:0.86422\n",
      "[224]\ttrain-mlogloss:0.73995\tval-mlogloss:0.86417\n",
      "[225]\ttrain-mlogloss:0.73970\tval-mlogloss:0.86409\n",
      "[226]\ttrain-mlogloss:0.73935\tval-mlogloss:0.86397\n",
      "[227]\ttrain-mlogloss:0.73916\tval-mlogloss:0.86391\n",
      "[228]\ttrain-mlogloss:0.73905\tval-mlogloss:0.86389\n",
      "[229]\ttrain-mlogloss:0.73899\tval-mlogloss:0.86384\n",
      "[230]\ttrain-mlogloss:0.73879\tval-mlogloss:0.86377\n",
      "[231]\ttrain-mlogloss:0.73851\tval-mlogloss:0.86371\n",
      "[232]\ttrain-mlogloss:0.73838\tval-mlogloss:0.86367\n",
      "[233]\ttrain-mlogloss:0.73822\tval-mlogloss:0.86361\n",
      "[234]\ttrain-mlogloss:0.73800\tval-mlogloss:0.86349\n",
      "[235]\ttrain-mlogloss:0.73788\tval-mlogloss:0.86343\n",
      "[236]\ttrain-mlogloss:0.73781\tval-mlogloss:0.86340\n",
      "[237]\ttrain-mlogloss:0.73754\tval-mlogloss:0.86329\n",
      "[238]\ttrain-mlogloss:0.73736\tval-mlogloss:0.86322\n",
      "[239]\ttrain-mlogloss:0.73696\tval-mlogloss:0.86314\n",
      "[240]\ttrain-mlogloss:0.73686\tval-mlogloss:0.86309\n",
      "[241]\ttrain-mlogloss:0.73663\tval-mlogloss:0.86304\n",
      "[242]\ttrain-mlogloss:0.73659\tval-mlogloss:0.86302\n",
      "[243]\ttrain-mlogloss:0.73644\tval-mlogloss:0.86297\n",
      "[244]\ttrain-mlogloss:0.73625\tval-mlogloss:0.86293\n",
      "[245]\ttrain-mlogloss:0.73614\tval-mlogloss:0.86291\n",
      "[246]\ttrain-mlogloss:0.73581\tval-mlogloss:0.86285\n",
      "[247]\ttrain-mlogloss:0.73567\tval-mlogloss:0.86279\n",
      "[248]\ttrain-mlogloss:0.73555\tval-mlogloss:0.86274\n",
      "[249]\ttrain-mlogloss:0.73535\tval-mlogloss:0.86269\n",
      "[250]\ttrain-mlogloss:0.73524\tval-mlogloss:0.86265\n",
      "[251]\ttrain-mlogloss:0.73515\tval-mlogloss:0.86262\n",
      "[252]\ttrain-mlogloss:0.73481\tval-mlogloss:0.86260\n",
      "[253]\ttrain-mlogloss:0.73470\tval-mlogloss:0.86256\n",
      "[254]\ttrain-mlogloss:0.73464\tval-mlogloss:0.86255\n",
      "[255]\ttrain-mlogloss:0.73444\tval-mlogloss:0.86249\n",
      "[256]\ttrain-mlogloss:0.73436\tval-mlogloss:0.86246\n",
      "[257]\ttrain-mlogloss:0.73428\tval-mlogloss:0.86240\n",
      "[258]\ttrain-mlogloss:0.73422\tval-mlogloss:0.86236\n",
      "[259]\ttrain-mlogloss:0.73420\tval-mlogloss:0.86235\n",
      "[260]\ttrain-mlogloss:0.73410\tval-mlogloss:0.86232\n",
      "[261]\ttrain-mlogloss:0.73402\tval-mlogloss:0.86229\n",
      "[262]\ttrain-mlogloss:0.73385\tval-mlogloss:0.86224\n",
      "[263]\ttrain-mlogloss:0.73371\tval-mlogloss:0.86215\n",
      "[264]\ttrain-mlogloss:0.73349\tval-mlogloss:0.86209\n",
      "[265]\ttrain-mlogloss:0.73337\tval-mlogloss:0.86208\n",
      "[266]\ttrain-mlogloss:0.73322\tval-mlogloss:0.86200\n",
      "[267]\ttrain-mlogloss:0.73314\tval-mlogloss:0.86195\n",
      "[268]\ttrain-mlogloss:0.73302\tval-mlogloss:0.86195\n",
      "[269]\ttrain-mlogloss:0.73289\tval-mlogloss:0.86191\n",
      "[270]\ttrain-mlogloss:0.73278\tval-mlogloss:0.86187\n",
      "[271]\ttrain-mlogloss:0.73264\tval-mlogloss:0.86183\n",
      "[272]\ttrain-mlogloss:0.73247\tval-mlogloss:0.86180\n",
      "[273]\ttrain-mlogloss:0.73237\tval-mlogloss:0.86173\n",
      "[274]\ttrain-mlogloss:0.73231\tval-mlogloss:0.86172\n",
      "[275]\ttrain-mlogloss:0.73222\tval-mlogloss:0.86173\n",
      "[276]\ttrain-mlogloss:0.73216\tval-mlogloss:0.86170\n",
      "[277]\ttrain-mlogloss:0.73205\tval-mlogloss:0.86165\n",
      "[278]\ttrain-mlogloss:0.73183\tval-mlogloss:0.86156\n",
      "[279]\ttrain-mlogloss:0.73167\tval-mlogloss:0.86151\n",
      "[280]\ttrain-mlogloss:0.73164\tval-mlogloss:0.86149\n",
      "[281]\ttrain-mlogloss:0.73157\tval-mlogloss:0.86146\n",
      "[282]\ttrain-mlogloss:0.73140\tval-mlogloss:0.86141\n",
      "[283]\ttrain-mlogloss:0.73128\tval-mlogloss:0.86135\n",
      "[284]\ttrain-mlogloss:0.73124\tval-mlogloss:0.86132\n",
      "[285]\ttrain-mlogloss:0.73107\tval-mlogloss:0.86123\n",
      "[286]\ttrain-mlogloss:0.73094\tval-mlogloss:0.86123\n",
      "[287]\ttrain-mlogloss:0.73090\tval-mlogloss:0.86122\n",
      "[288]\ttrain-mlogloss:0.73078\tval-mlogloss:0.86119\n",
      "[289]\ttrain-mlogloss:0.73059\tval-mlogloss:0.86117\n",
      "[290]\ttrain-mlogloss:0.73054\tval-mlogloss:0.86115\n",
      "[291]\ttrain-mlogloss:0.73053\tval-mlogloss:0.86114\n",
      "[292]\ttrain-mlogloss:0.73042\tval-mlogloss:0.86110\n",
      "[293]\ttrain-mlogloss:0.73031\tval-mlogloss:0.86104\n",
      "[294]\ttrain-mlogloss:0.73021\tval-mlogloss:0.86101\n",
      "[295]\ttrain-mlogloss:0.73016\tval-mlogloss:0.86100\n",
      "[296]\ttrain-mlogloss:0.73002\tval-mlogloss:0.86098\n",
      "[297]\ttrain-mlogloss:0.72996\tval-mlogloss:0.86095\n",
      "[298]\ttrain-mlogloss:0.72983\tval-mlogloss:0.86088\n",
      "[299]\ttrain-mlogloss:0.72972\tval-mlogloss:0.86082\n",
      "[300]\ttrain-mlogloss:0.72959\tval-mlogloss:0.86076\n",
      "[301]\ttrain-mlogloss:0.72956\tval-mlogloss:0.86074\n",
      "[302]\ttrain-mlogloss:0.72953\tval-mlogloss:0.86074\n",
      "[303]\ttrain-mlogloss:0.72942\tval-mlogloss:0.86072\n",
      "[304]\ttrain-mlogloss:0.72940\tval-mlogloss:0.86071\n",
      "[305]\ttrain-mlogloss:0.72935\tval-mlogloss:0.86069\n",
      "[306]\ttrain-mlogloss:0.72925\tval-mlogloss:0.86064\n",
      "[307]\ttrain-mlogloss:0.72916\tval-mlogloss:0.86062\n",
      "[308]\ttrain-mlogloss:0.72894\tval-mlogloss:0.86055\n",
      "[309]\ttrain-mlogloss:0.72877\tval-mlogloss:0.86052\n",
      "[310]\ttrain-mlogloss:0.72874\tval-mlogloss:0.86052\n",
      "[311]\ttrain-mlogloss:0.72872\tval-mlogloss:0.86052\n",
      "[312]\ttrain-mlogloss:0.72864\tval-mlogloss:0.86050\n",
      "[313]\ttrain-mlogloss:0.72853\tval-mlogloss:0.86047\n",
      "[314]\ttrain-mlogloss:0.72835\tval-mlogloss:0.86042\n",
      "[315]\ttrain-mlogloss:0.72834\tval-mlogloss:0.86041\n",
      "[316]\ttrain-mlogloss:0.72827\tval-mlogloss:0.86040\n",
      "[317]\ttrain-mlogloss:0.72824\tval-mlogloss:0.86041\n",
      "[318]\ttrain-mlogloss:0.72816\tval-mlogloss:0.86036\n",
      "[319]\ttrain-mlogloss:0.72814\tval-mlogloss:0.86037\n",
      "[320]\ttrain-mlogloss:0.72796\tval-mlogloss:0.86033\n",
      "[321]\ttrain-mlogloss:0.72791\tval-mlogloss:0.86032\n",
      "[322]\ttrain-mlogloss:0.72790\tval-mlogloss:0.86031\n",
      "[323]\ttrain-mlogloss:0.72788\tval-mlogloss:0.86031\n",
      "[324]\ttrain-mlogloss:0.72786\tval-mlogloss:0.86031\n",
      "[325]\ttrain-mlogloss:0.72774\tval-mlogloss:0.86026\n",
      "[326]\ttrain-mlogloss:0.72765\tval-mlogloss:0.86023\n",
      "[327]\ttrain-mlogloss:0.72761\tval-mlogloss:0.86022\n",
      "[328]\ttrain-mlogloss:0.72752\tval-mlogloss:0.86019\n",
      "[329]\ttrain-mlogloss:0.72746\tval-mlogloss:0.86017\n",
      "[330]\ttrain-mlogloss:0.72742\tval-mlogloss:0.86017\n",
      "[331]\ttrain-mlogloss:0.72731\tval-mlogloss:0.86016\n",
      "[332]\ttrain-mlogloss:0.72726\tval-mlogloss:0.86014\n",
      "[333]\ttrain-mlogloss:0.72713\tval-mlogloss:0.86012\n",
      "[334]\ttrain-mlogloss:0.72713\tval-mlogloss:0.86012\n",
      "[335]\ttrain-mlogloss:0.72713\tval-mlogloss:0.86012\n",
      "[336]\ttrain-mlogloss:0.72710\tval-mlogloss:0.86009\n",
      "[337]\ttrain-mlogloss:0.72695\tval-mlogloss:0.86008\n",
      "[338]\ttrain-mlogloss:0.72694\tval-mlogloss:0.86006\n",
      "[339]\ttrain-mlogloss:0.72693\tval-mlogloss:0.86006\n",
      "[340]\ttrain-mlogloss:0.72692\tval-mlogloss:0.86005\n",
      "[341]\ttrain-mlogloss:0.72690\tval-mlogloss:0.86006\n",
      "[342]\ttrain-mlogloss:0.72685\tval-mlogloss:0.86006\n",
      "[343]\ttrain-mlogloss:0.72685\tval-mlogloss:0.86006\n",
      "[344]\ttrain-mlogloss:0.72673\tval-mlogloss:0.86001\n",
      "[345]\ttrain-mlogloss:0.72660\tval-mlogloss:0.85997\n",
      "[346]\ttrain-mlogloss:0.72659\tval-mlogloss:0.85997\n",
      "[347]\ttrain-mlogloss:0.72653\tval-mlogloss:0.85995\n",
      "[348]\ttrain-mlogloss:0.72647\tval-mlogloss:0.85992\n",
      "[349]\ttrain-mlogloss:0.72641\tval-mlogloss:0.85990\n",
      "[350]\ttrain-mlogloss:0.72637\tval-mlogloss:0.85988\n",
      "[351]\ttrain-mlogloss:0.72627\tval-mlogloss:0.85989\n",
      "[352]\ttrain-mlogloss:0.72624\tval-mlogloss:0.85988\n",
      "[353]\ttrain-mlogloss:0.72621\tval-mlogloss:0.85987\n",
      "[354]\ttrain-mlogloss:0.72621\tval-mlogloss:0.85987\n",
      "[355]\ttrain-mlogloss:0.72617\tval-mlogloss:0.85986\n",
      "[356]\ttrain-mlogloss:0.72614\tval-mlogloss:0.85987\n",
      "[357]\ttrain-mlogloss:0.72604\tval-mlogloss:0.85984\n",
      "[358]\ttrain-mlogloss:0.72588\tval-mlogloss:0.85981\n",
      "[359]\ttrain-mlogloss:0.72585\tval-mlogloss:0.85979\n",
      "[360]\ttrain-mlogloss:0.72579\tval-mlogloss:0.85979\n",
      "[361]\ttrain-mlogloss:0.72577\tval-mlogloss:0.85979\n",
      "[362]\ttrain-mlogloss:0.72577\tval-mlogloss:0.85978\n",
      "[363]\ttrain-mlogloss:0.72573\tval-mlogloss:0.85977\n",
      "[364]\ttrain-mlogloss:0.72572\tval-mlogloss:0.85976\n",
      "[365]\ttrain-mlogloss:0.72563\tval-mlogloss:0.85971\n",
      "[366]\ttrain-mlogloss:0.72562\tval-mlogloss:0.85970\n",
      "[367]\ttrain-mlogloss:0.72554\tval-mlogloss:0.85970\n",
      "[368]\ttrain-mlogloss:0.72540\tval-mlogloss:0.85966\n",
      "[369]\ttrain-mlogloss:0.72534\tval-mlogloss:0.85965\n",
      "[370]\ttrain-mlogloss:0.72519\tval-mlogloss:0.85961\n",
      "[371]\ttrain-mlogloss:0.72519\tval-mlogloss:0.85961\n",
      "[372]\ttrain-mlogloss:0.72518\tval-mlogloss:0.85960\n",
      "[373]\ttrain-mlogloss:0.72513\tval-mlogloss:0.85957\n",
      "[374]\ttrain-mlogloss:0.72508\tval-mlogloss:0.85957\n",
      "[375]\ttrain-mlogloss:0.72507\tval-mlogloss:0.85957\n",
      "[376]\ttrain-mlogloss:0.72501\tval-mlogloss:0.85954\n",
      "[377]\ttrain-mlogloss:0.72499\tval-mlogloss:0.85953\n",
      "[378]\ttrain-mlogloss:0.72486\tval-mlogloss:0.85954\n",
      "[379]\ttrain-mlogloss:0.72484\tval-mlogloss:0.85952\n",
      "[380]\ttrain-mlogloss:0.72484\tval-mlogloss:0.85951\n",
      "[381]\ttrain-mlogloss:0.72478\tval-mlogloss:0.85949\n",
      "[382]\ttrain-mlogloss:0.72477\tval-mlogloss:0.85949\n",
      "[383]\ttrain-mlogloss:0.72467\tval-mlogloss:0.85950\n",
      "[384]\ttrain-mlogloss:0.72460\tval-mlogloss:0.85950\n",
      "[385]\ttrain-mlogloss:0.72460\tval-mlogloss:0.85950\n",
      "[386]\ttrain-mlogloss:0.72459\tval-mlogloss:0.85949\n",
      "[387]\ttrain-mlogloss:0.72459\tval-mlogloss:0.85949\n",
      "[388]\ttrain-mlogloss:0.72455\tval-mlogloss:0.85947\n",
      "[389]\ttrain-mlogloss:0.72450\tval-mlogloss:0.85944\n",
      "[390]\ttrain-mlogloss:0.72446\tval-mlogloss:0.85944\n",
      "[391]\ttrain-mlogloss:0.72445\tval-mlogloss:0.85943\n",
      "[392]\ttrain-mlogloss:0.72439\tval-mlogloss:0.85943\n",
      "[393]\ttrain-mlogloss:0.72439\tval-mlogloss:0.85943\n",
      "[394]\ttrain-mlogloss:0.72437\tval-mlogloss:0.85941\n",
      "[395]\ttrain-mlogloss:0.72430\tval-mlogloss:0.85938\n",
      "[396]\ttrain-mlogloss:0.72429\tval-mlogloss:0.85938\n",
      "[397]\ttrain-mlogloss:0.72425\tval-mlogloss:0.85936\n",
      "[398]\ttrain-mlogloss:0.72419\tval-mlogloss:0.85935\n",
      "[399]\ttrain-mlogloss:0.72410\tval-mlogloss:0.85933\n",
      "[400]\ttrain-mlogloss:0.72404\tval-mlogloss:0.85934\n",
      "[401]\ttrain-mlogloss:0.72401\tval-mlogloss:0.85932\n",
      "[402]\ttrain-mlogloss:0.72400\tval-mlogloss:0.85932\n",
      "[403]\ttrain-mlogloss:0.72396\tval-mlogloss:0.85930\n",
      "[404]\ttrain-mlogloss:0.72385\tval-mlogloss:0.85931\n",
      "[405]\ttrain-mlogloss:0.72383\tval-mlogloss:0.85930\n",
      "[406]\ttrain-mlogloss:0.72377\tval-mlogloss:0.85927\n",
      "[407]\ttrain-mlogloss:0.72376\tval-mlogloss:0.85928\n",
      "[408]\ttrain-mlogloss:0.72373\tval-mlogloss:0.85928\n",
      "[409]\ttrain-mlogloss:0.72366\tval-mlogloss:0.85926\n",
      "[410]\ttrain-mlogloss:0.72366\tval-mlogloss:0.85926\n",
      "[411]\ttrain-mlogloss:0.72358\tval-mlogloss:0.85925\n",
      "[412]\ttrain-mlogloss:0.72356\tval-mlogloss:0.85923\n",
      "[413]\ttrain-mlogloss:0.72352\tval-mlogloss:0.85922\n",
      "[414]\ttrain-mlogloss:0.72350\tval-mlogloss:0.85922\n",
      "[415]\ttrain-mlogloss:0.72342\tval-mlogloss:0.85920\n",
      "[416]\ttrain-mlogloss:0.72340\tval-mlogloss:0.85919\n",
      "[417]\ttrain-mlogloss:0.72334\tval-mlogloss:0.85917\n",
      "[418]\ttrain-mlogloss:0.72332\tval-mlogloss:0.85916\n",
      "[419]\ttrain-mlogloss:0.72322\tval-mlogloss:0.85912\n",
      "[420]\ttrain-mlogloss:0.72320\tval-mlogloss:0.85912\n",
      "[421]\ttrain-mlogloss:0.72320\tval-mlogloss:0.85912\n",
      "[422]\ttrain-mlogloss:0.72319\tval-mlogloss:0.85911\n",
      "[423]\ttrain-mlogloss:0.72309\tval-mlogloss:0.85907\n",
      "[424]\ttrain-mlogloss:0.72307\tval-mlogloss:0.85906\n",
      "[425]\ttrain-mlogloss:0.72307\tval-mlogloss:0.85906\n",
      "[426]\ttrain-mlogloss:0.72307\tval-mlogloss:0.85906\n",
      "[427]\ttrain-mlogloss:0.72307\tval-mlogloss:0.85906\n",
      "[428]\ttrain-mlogloss:0.72305\tval-mlogloss:0.85905\n",
      "[429]\ttrain-mlogloss:0.72302\tval-mlogloss:0.85903\n",
      "[430]\ttrain-mlogloss:0.72300\tval-mlogloss:0.85902\n",
      "[431]\ttrain-mlogloss:0.72296\tval-mlogloss:0.85902\n",
      "[432]\ttrain-mlogloss:0.72293\tval-mlogloss:0.85901\n",
      "[433]\ttrain-mlogloss:0.72292\tval-mlogloss:0.85902\n",
      "[434]\ttrain-mlogloss:0.72292\tval-mlogloss:0.85902\n",
      "[435]\ttrain-mlogloss:0.72288\tval-mlogloss:0.85900\n",
      "[436]\ttrain-mlogloss:0.72287\tval-mlogloss:0.85899\n",
      "[437]\ttrain-mlogloss:0.72287\tval-mlogloss:0.85899\n",
      "[438]\ttrain-mlogloss:0.72278\tval-mlogloss:0.85897\n",
      "[439]\ttrain-mlogloss:0.72272\tval-mlogloss:0.85898\n",
      "[440]\ttrain-mlogloss:0.72272\tval-mlogloss:0.85897\n",
      "[441]\ttrain-mlogloss:0.72270\tval-mlogloss:0.85898\n",
      "[442]\ttrain-mlogloss:0.72269\tval-mlogloss:0.85897\n",
      "[443]\ttrain-mlogloss:0.72263\tval-mlogloss:0.85896\n",
      "[444]\ttrain-mlogloss:0.72263\tval-mlogloss:0.85896\n",
      "[445]\ttrain-mlogloss:0.72259\tval-mlogloss:0.85894\n",
      "[446]\ttrain-mlogloss:0.72259\tval-mlogloss:0.85894\n",
      "[447]\ttrain-mlogloss:0.72255\tval-mlogloss:0.85893\n",
      "[448]\ttrain-mlogloss:0.72254\tval-mlogloss:0.85893\n",
      "[449]\ttrain-mlogloss:0.72251\tval-mlogloss:0.85892\n",
      "[450]\ttrain-mlogloss:0.72247\tval-mlogloss:0.85893\n",
      "[451]\ttrain-mlogloss:0.72247\tval-mlogloss:0.85893\n",
      "[452]\ttrain-mlogloss:0.72244\tval-mlogloss:0.85892\n",
      "[453]\ttrain-mlogloss:0.72240\tval-mlogloss:0.85891\n",
      "[454]\ttrain-mlogloss:0.72231\tval-mlogloss:0.85890\n",
      "[455]\ttrain-mlogloss:0.72231\tval-mlogloss:0.85890\n",
      "[456]\ttrain-mlogloss:0.72225\tval-mlogloss:0.85889\n",
      "[457]\ttrain-mlogloss:0.72225\tval-mlogloss:0.85889\n",
      "[458]\ttrain-mlogloss:0.72224\tval-mlogloss:0.85889\n",
      "[459]\ttrain-mlogloss:0.72220\tval-mlogloss:0.85886\n",
      "[460]\ttrain-mlogloss:0.72220\tval-mlogloss:0.85886\n",
      "[461]\ttrain-mlogloss:0.72220\tval-mlogloss:0.85886\n",
      "[462]\ttrain-mlogloss:0.72219\tval-mlogloss:0.85886\n",
      "[463]\ttrain-mlogloss:0.72219\tval-mlogloss:0.85886\n",
      "[464]\ttrain-mlogloss:0.72214\tval-mlogloss:0.85885\n",
      "[465]\ttrain-mlogloss:0.72214\tval-mlogloss:0.85885\n",
      "[466]\ttrain-mlogloss:0.72214\tval-mlogloss:0.85885\n",
      "[467]\ttrain-mlogloss:0.72213\tval-mlogloss:0.85884\n",
      "[468]\ttrain-mlogloss:0.72207\tval-mlogloss:0.85882\n",
      "[469]\ttrain-mlogloss:0.72206\tval-mlogloss:0.85881\n",
      "[470]\ttrain-mlogloss:0.72206\tval-mlogloss:0.85881\n",
      "[471]\ttrain-mlogloss:0.72206\tval-mlogloss:0.85881\n",
      "[472]\ttrain-mlogloss:0.72206\tval-mlogloss:0.85879\n",
      "[473]\ttrain-mlogloss:0.72205\tval-mlogloss:0.85879\n",
      "[474]\ttrain-mlogloss:0.72205\tval-mlogloss:0.85879\n",
      "[475]\ttrain-mlogloss:0.72204\tval-mlogloss:0.85879\n",
      "[476]\ttrain-mlogloss:0.72201\tval-mlogloss:0.85877\n",
      "[477]\ttrain-mlogloss:0.72201\tval-mlogloss:0.85876\n",
      "[478]\ttrain-mlogloss:0.72200\tval-mlogloss:0.85875\n",
      "[479]\ttrain-mlogloss:0.72199\tval-mlogloss:0.85875\n",
      "[480]\ttrain-mlogloss:0.72198\tval-mlogloss:0.85875\n",
      "[481]\ttrain-mlogloss:0.72192\tval-mlogloss:0.85876\n",
      "[482]\ttrain-mlogloss:0.72189\tval-mlogloss:0.85876\n",
      "[483]\ttrain-mlogloss:0.72187\tval-mlogloss:0.85873\n",
      "[484]\ttrain-mlogloss:0.72181\tval-mlogloss:0.85872\n",
      "[485]\ttrain-mlogloss:0.72176\tval-mlogloss:0.85870\n",
      "[486]\ttrain-mlogloss:0.72176\tval-mlogloss:0.85870\n",
      "[487]\ttrain-mlogloss:0.72172\tval-mlogloss:0.85870\n",
      "[488]\ttrain-mlogloss:0.72172\tval-mlogloss:0.85870\n",
      "[489]\ttrain-mlogloss:0.72172\tval-mlogloss:0.85870\n",
      "[490]\ttrain-mlogloss:0.72169\tval-mlogloss:0.85869\n",
      "[491]\ttrain-mlogloss:0.72167\tval-mlogloss:0.85868\n",
      "[492]\ttrain-mlogloss:0.72166\tval-mlogloss:0.85867\n",
      "[493]\ttrain-mlogloss:0.72166\tval-mlogloss:0.85867\n",
      "[494]\ttrain-mlogloss:0.72165\tval-mlogloss:0.85866\n",
      "[495]\ttrain-mlogloss:0.72165\tval-mlogloss:0.85866\n",
      "[496]\ttrain-mlogloss:0.72162\tval-mlogloss:0.85866\n",
      "[497]\ttrain-mlogloss:0.72161\tval-mlogloss:0.85865\n",
      "[498]\ttrain-mlogloss:0.72157\tval-mlogloss:0.85865\n",
      "[499]\ttrain-mlogloss:0.72157\tval-mlogloss:0.85865\n",
      "[500]\ttrain-mlogloss:0.72150\tval-mlogloss:0.85863\n",
      "[501]\ttrain-mlogloss:0.72150\tval-mlogloss:0.85863\n",
      "[502]\ttrain-mlogloss:0.72147\tval-mlogloss:0.85863\n",
      "[503]\ttrain-mlogloss:0.72146\tval-mlogloss:0.85862\n",
      "[504]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85862\n",
      "[505]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85862\n",
      "[506]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85862\n",
      "[507]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85862\n",
      "[508]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85862\n",
      "[509]\ttrain-mlogloss:0.72145\tval-mlogloss:0.85861\n",
      "[510]\ttrain-mlogloss:0.72143\tval-mlogloss:0.85860\n",
      "[511]\ttrain-mlogloss:0.72141\tval-mlogloss:0.85860\n",
      "[512]\ttrain-mlogloss:0.72141\tval-mlogloss:0.85860\n",
      "[513]\ttrain-mlogloss:0.72140\tval-mlogloss:0.85859\n",
      "[514]\ttrain-mlogloss:0.72137\tval-mlogloss:0.85860\n",
      "[515]\ttrain-mlogloss:0.72137\tval-mlogloss:0.85860\n",
      "[516]\ttrain-mlogloss:0.72137\tval-mlogloss:0.85860\n",
      "[517]\ttrain-mlogloss:0.72137\tval-mlogloss:0.85861\n",
      "[518]\ttrain-mlogloss:0.72134\tval-mlogloss:0.85860\n",
      "[519]\ttrain-mlogloss:0.72134\tval-mlogloss:0.85860\n",
      "[520]\ttrain-mlogloss:0.72133\tval-mlogloss:0.85860\n",
      "[521]\ttrain-mlogloss:0.72126\tval-mlogloss:0.85860\n",
      "[522]\ttrain-mlogloss:0.72126\tval-mlogloss:0.85860\n",
      "Training Multiclass Logarithmic Loss: 0.721262802016378\n",
      "Validation Multiclass Logarithmic Loss: 0.8585973113891668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y, test_size=0.3, random_state=42)\n",
    "\n",
    "adjusted_params = {\n",
    "    'colsample_bytree': 0.6,  # Slightly lower to increase regularization\n",
    "    'device': 'cuda',         # Keeping it as is for GPU acceleration\n",
    "    'gamma': 3.5,               # Lowered to reduce overfitting by making the algorithm conservative\n",
    "    'learning_rate': 0.05,    # Lowered for finer steps towards convergence\n",
    "    'max_depth': 7,           # Slightly reduced to control complexity and overfitting\n",
    "    'n_estimators': 1000,     # Increased to compensate for the lower learning rate\n",
    "    'subsample': 0.8,         # Increased to use a bit more data for each tree\n",
    "    'min_child_weight': 5,    # Lowered to consider splits with fewer samples\n",
    "    'alpha': 1.5,               # Slightly increased L1 regularization\n",
    "    'lambda': 2.5,              # Slightly reduced L2 regularization to balance with L1\n",
    "    'objective': 'multi:softprob',  # No change, appropriate for multi-class classification\n",
    "    'num_class': 5,           # Ensure this matches your actual number of classes\n",
    "    'eval_metric': 'mlogloss',  # No change, as we're focusing on log loss\n",
    "}\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(adjusted_params, dtrain, num_boost_round=800, evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=10, evals_result=evals_result, verbose_eval=True)\n",
    "\n",
    "# Evaluate and print the final training and validation loss\n",
    "train_last_eval = evals_result['train']['mlogloss'][-1]\n",
    "val_last_eval = evals_result['val']['mlogloss'][-1]\n",
    "\n",
    "print(f\"Training Multiclass Logarithmic Loss: {train_last_eval}\")\n",
    "print(f\"Validation Multiclass Logarithmic Loss: {val_last_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ae6668498e2687f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-29T19:48:21.430640Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(df_test, enable_categorical=True)\n",
    "y_test_probs = bst.predict(dtest)\n",
    "class_order = [0, 1, 2, 3, 4]\n",
    "class_mapping = {class_label: f\"Class{class_label}\" for class_label in class_order}\n",
    "\n",
    "submission_df = pd.DataFrame(y_test_probs, columns=class_mapping.values())\n",
    "submission_df.columns = ['no answer', 'very important', 'quite important', 'not important', 'not at all important']\n",
    "submission_df.insert(0, 'id', df_test.index)\n",
    "\n",
    "# Save the submission file\n",
    "submission_file = ('plzz_submission.csv')\n",
    "submission_df.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a62cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
