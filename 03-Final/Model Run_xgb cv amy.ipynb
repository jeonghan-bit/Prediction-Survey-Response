{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T17:36:11.200565Z",
     "start_time": "2024-03-29T17:36:06.376492Z"
    }
   },
   "outputs": [],
   "source": [
    "from EDA_script import *\n",
    "# train_x_raw = pd.read_csv(\"../01-Data/X_train.csv\", low_memory = True, index_col=0)\n",
    "# train_y_raw = pd.read_csv(\"../01-Data/y_train.csv\", low_memory = True, index_col=0)\n",
    "# test_x_raw = pd.read_csv(\"../01-Data/X_test.csv\", low_memory=True, index_col=0)\n",
    "\n",
    "# df_train = pd.DataFrame(train_x_raw)\n",
    "# df_test = pd.DataFrame(test_x_raw)\n",
    "# df_y = pd.DataFrame(train_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646daeb6719b84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Model Run\n",
    "##  xgBoost model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65d6aaf6aa41b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Remap labels\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d5d02b4bf4f48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e53c89851a293",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_cv_score(max_depth, gamma, colsample_bytree, subsample, eta, reg_lambda, reg_alpha, min_child_weight):\n",
    "    \"\"\"\n",
    "    Computes the cross-validated log loss for given hyperparameter settings using Stratified K-Fold.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'subsample': subsample,\n",
    "        'eta': eta,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 5,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'lambda': reg_lambda,\n",
    "        'alpha': reg_alpha,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    log_loss_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(df_train, df_y):\n",
    "        xgb_train = xgb.DMatrix(df_train.iloc[train_index], label=df_y.iloc[train_index])\n",
    "        xgb_valid = xgb.DMatrix(df_train.iloc[test_index], label=df_y.iloc[test_index])\n",
    "\n",
    "        watchlist = [(xgb_train, 'train'), (xgb_valid, 'eval')]\n",
    "        model = xgb.train(params, xgb_train, num_boost_round=1000, evals=watchlist, early_stopping_rounds=50, verbose_eval=False)\n",
    "        \n",
    "        preds = model.predict(xgb_valid, ntree_limit=model.best_ntree_limit)\n",
    "        log_loss_score = log_loss(df_y.iloc[test_index], preds, labels=list(range(5)))\n",
    "        log_loss_scores.append(log_loss_score)\n",
    "\n",
    "    return -np.mean(log_loss_scores)\n",
    "\n",
    "# Define the hyperparameter bounds\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'gamma': (0, 1),\n",
    "    'colsample_bytree': (0.3, 0.9),\n",
    "    'subsample': (0.3, 0.9),\n",
    "    'eta': (0.01, 0.3),\n",
    "    'reg_lambda': (1, 5),\n",
    "    'reg_alpha': (0, 1),\n",
    "    'min_child_weight': (1, 6),\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "optimizer = BayesianOptimization(f=xgb_cv_score, pbounds=pbounds, random_state=42, verbose=2)\n",
    "optimizer.maximize(init_points=10, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ffd766a3e1d0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CV xgboost train with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10d0df175935aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Best parameters from optimization\n",
    "best_params = {\n",
    "    'max_depth': int(optimizer.max['params']['max_depth']),\n",
    "    'gamma': optimizer.max['params']['gamma'],\n",
    "    'colsample_bytree': optimizer.max['params']['colsample_bytree'],\n",
    "    'subsample': optimizer.max['params']['subsample'],\n",
    "    'eta': optimizer.max['params']['eta'],\n",
    "    'lambda': optimizer.max['params']['reg_lambda'],\n",
    "    'alpha': optimizer.max['params']['reg_alpha'],\n",
    "    'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(best_params, dtrain, num_boost_round=1000, evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=10, evals_result=evals_result, verbose_eval=True)\n",
    "\n",
    "# Evaluate and print the final training and validation loss\n",
    "train_last_eval = evals_result['train']['mlogloss'][-1]\n",
    "val_last_eval = evals_result['val']['mlogloss'][-1]\n",
    "\n",
    "print(f\"Training Multiclass Logarithmic Loss: {train_last_eval}\")\n",
    "print(f\"Validation Multiclass Logarithmic Loss: {val_last_eval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527521c5801211",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate Submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae6668498e2687f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T18:16:07.470954Z",
     "start_time": "2024-03-29T18:16:07.457740Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# submission_df = pd.DataFrame(y_test_probs, columns=class_mapping.values())\n",
    "# submission_df.columns = ['no answer', 'very important', 'quite important', 'not important', 'not at all important']\n",
    "# submission_df.insert(0, 'id', df_test.index)\n",
    "# \n",
    "# # Save the submission file\n",
    "# submission_file = ('submission.csv')\n",
    "# submission_df.to_csv(submission_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
