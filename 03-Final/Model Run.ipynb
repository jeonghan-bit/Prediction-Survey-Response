{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:30:07.403370Z",
     "start_time": "2024-03-26T17:30:05.589116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set options\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "train_x_raw = pd.read_csv(\"../01-Data/X_train.csv\", low_memory = True, index_col=0)\n",
    "train_y_raw = pd.read_csv(\"../01-Data/y_train.csv\", low_memory = True, index_col=0)\n",
    "test_x_raw = pd.read_csv(\"../01-Data/X_test.csv\", low_memory=True, index_col=0)\n",
    "\n",
    "df_train = pd.DataFrame(train_x_raw)\n",
    "df_test = pd.DataFrame(test_x_raw)\n",
    "df_y = pd.DataFrame(train_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c05a8884dfda7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add all of the preprocessing below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39710aab3026879d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 1 - 146 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d335519eab176102",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:30:09.240763Z",
     "start_time": "2024-03-26T17:30:09.181321Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['c_abrv', 'f46_IT', 'v72_DE', 'v73_DE', 'v74_DE', 'v75_DE', 'v76_DE', 'v77_DE', 'v78_DE', 'v79_DE']\n",
    "df_train.drop(columns=columns_to_drop, inplace=True)\n",
    "df_test.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef956e35e6f124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 147 - 292 Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f908a39187ffa8bf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:30:10.930397Z",
     "start_time": "2024-03-26T17:30:10.915398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v30c', 'v45c', 'v133_11c', 'v134_11c', 'v135_11c', 'v136_11c', 'v137_11c', 'v138_11c', 'v139_11c', 'v140_11c', 'v141_11c']\n",
      "['age_r', 'v228b_r', 'v231b_r', 'v233b_r', 'v239_r', 'v242_r', 'v243_r', 'v251b_r', 'v252_r', 'v261_r', 'v262_r', 'v263_r', 'v276_r', 'v278c_r', 'v279c_r', 'v279d_r', 'v281a_r']\n"
     ]
    }
   ],
   "source": [
    "### Function to find the targeted colname\n",
    "def find_colname(data, target):\n",
    "    temp = []\n",
    "    for varname in data.columns:\n",
    "        if varname.endswith(target):\n",
    "            temp.append(varname)\n",
    "    return(temp)\n",
    "\n",
    "merge_colname = find_colname(train_x_raw, '_11c')\n",
    "print(find_colname(train_x_raw, 'c'))\n",
    "print(find_colname(train_x_raw, '_r'))\n",
    "\n",
    "def merge_columns(dat, colname):\n",
    "    for name in colname:\n",
    "        name_org = name.replace(\"_11c\", \"\")\n",
    "        dat.loc[dat[name_org] == -4, name_org] = dat.loc[dat[name_org] == -4, name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d251374774258cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 293 - 438 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74a294a5eb2c211d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:30:13.847779Z",
     "start_time": "2024-03-26T17:30:13.099198Z"
    }
   },
   "outputs": [],
   "source": [
    "## removed string type data\n",
    "df_train.drop('v228b', inplace=True, axis=1) \n",
    "df_test.drop('v228b', inplace=True, axis=1) \n",
    "\n",
    "df_train.fillna({'v228b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v228b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v231b', inplace=True, axis=1) \n",
    "df_test.drop('v231b', inplace=True, axis=1)\n",
    "\n",
    "df_train.fillna({'v231b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v231b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v233b', inplace=True, axis=1)\n",
    "df_test.drop('v233b', inplace=True, axis=1)\n",
    "\n",
    "df_train.fillna({'v233b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v233b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v251b', inplace=True, axis=1)\n",
    "df_test.drop('v251b', inplace=True, axis=1) \n",
    "\n",
    "df_train.fillna({'v251b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v251b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('f252_edulvlb_CH', inplace=True, axis=1)\n",
    "df_test.drop('f252_edulvlb_CH', inplace=True, axis=1)\n",
    "\n",
    "## removed the column having 'DE'\n",
    "df_train.drop(list(df_train.filter(regex='DE')), axis=1, inplace=True)\n",
    "df_test.drop(list(df_test.filter(regex='DE')), axis=1, inplace=True)\n",
    "\n",
    "## removed the column having 'GB'\n",
    "df_train.drop(list(df_train.filter(regex='GB')), axis=1, inplace=True)\n",
    "df_test.drop(list(df_test.filter(regex='GB')), axis=1, inplace=True)\n",
    "\n",
    "df_train.drop('v281a', inplace=True, axis=1)\n",
    "df_test.drop('v281a', inplace=True, axis=1)\n",
    "\n",
    "df_train.drop('v275b_N2', inplace=True, axis=1) \n",
    "df_test.drop('v275b_N2', inplace=True, axis=1) \n",
    "\n",
    "df_train.drop('v275b_N1', inplace=True, axis=1) \n",
    "df_test.drop('v275b_N1', inplace=True, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646daeb6719b84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Model Run\n",
    "##  xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11438, 9600]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m y_test_probs \u001B[38;5;241m=\u001B[39m bst\u001B[38;5;241m.\u001B[39mpredict(dtest)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Compute Multiclass Logarithmic Loss\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m val_log_loss \u001B[38;5;241m=\u001B[39m \u001B[43mlog_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test_probs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Multiclass Logarithmic Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_log_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     28\u001B[0m class_order \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2919\u001B[0m, in \u001B[0;36mlog_loss\u001B[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001B[0m\n\u001B[0;32m   2908\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2909\u001B[0m     \u001B[38;5;66;03m# TODO: Remove user defined eps in 1.5\u001B[39;00m\n\u001B[0;32m   2910\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   2911\u001B[0m         (\n\u001B[0;32m   2912\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting the eps parameter is deprecated and will \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2916\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m   2917\u001B[0m     )\n\u001B[1;32m-> 2919\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2920\u001B[0m lb \u001B[38;5;241m=\u001B[39m LabelBinarizer()\n\u001B[0;32m   2922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    455\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    458\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    459\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    460\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [11438, 9600]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)\n",
    "\n",
    "dtrain = xgb.DMatrix(df_train, label=df_y, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(df_test, enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,\n",
    "    'eval_metric': 'mlogloss',\n",
    "}\n",
    "num_boost_round = 500\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round)\n",
    "y_test_probs = bst.predict(dtest)\n",
    "\n",
    "# Compute Multiclass Logarithmic Loss\n",
    "#val_log_loss = log_loss(y_val, y_test_probs)\n",
    "#print(f\"Validation Multiclass Logarithmic Loss: {val_log_loss}\")\n",
    "\n",
    "class_order = [0, 1, 2, 3, 4]\n",
    "class_mapping = {class_label: f\"Class_{class_label}\" for class_label in class_order}\n",
    "\n",
    "submission_df = pd.DataFrame(y_test_probs, columns=class_mapping.values())\n",
    "submission_df.columns = ['no answer', 'very important', 'quite important', 'not important', 'not at all important']\n",
    "submission_df.insert(0, 'id', df_test.index)\n",
    "\n",
    "# Save the submission file\n",
    "#submission_file = ('submission.csv')\n",
    "#submission_df.to_csv(submission_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:42:34.552996Z",
     "start_time": "2024-03-26T17:41:02.493377Z"
    }
   },
   "id": "5fbfcd048b55d241",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Random Forest with RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29cc48e8c64c78a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV] END ......................max_depth=7, n_estimators=448; total time=  14.2s\n",
      "[CV] END ......................max_depth=7, n_estimators=448; total time=  14.0s\n",
      "Best parameters: {'max_depth': 7, 'n_estimators': 448}\n",
      "Best score: -1.0336114563312448\n",
      "Test Multiclass Logarithmic Loss: 1.0136927082560196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y.squeeze(), test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the distribution of parameters\n",
    "param_distributions = {\n",
    "    'n_estimators': stats.randint(100, 500),  \n",
    "    'max_depth': stats.randint(4, 10), \n",
    "    # Include other parameters\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV / change n_iter, cv\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=1, cv=2, scoring='neg_log_loss', random_state=42, verbose=2)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Best score achieved\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "# RandomForestClassifier with the best parameters\n",
    "best_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "best_model.fit(df_train, df_y.squeeze())  \n",
    "\n",
    "y_test_probs = best_model.predict_proba(X_val)  \n",
    "test_log_loss = log_loss(y_val, y_test_probs)  \n",
    "print(f\"Test Multiclass Logarithmic Loss: {test_log_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T17:34:36.154445Z",
     "start_time": "2024-03-26T17:33:06.284440Z"
    }
   },
   "id": "9a24b215b48afc4d",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73377ae83e283639"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
