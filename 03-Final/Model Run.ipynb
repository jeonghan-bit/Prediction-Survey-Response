{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:05:09.052559Z",
     "start_time": "2024-03-27T18:05:06.992812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set options\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "train_x_raw = pd.read_csv(\"../01-Data/X_train.csv\", low_memory = True, index_col=0)\n",
    "train_y_raw = pd.read_csv(\"../01-Data/y_train.csv\", low_memory = True, index_col=0)\n",
    "test_x_raw = pd.read_csv(\"../01-Data/X_test.csv\", low_memory=True, index_col=0)\n",
    "\n",
    "df_train = pd.DataFrame(train_x_raw)\n",
    "df_test = pd.DataFrame(test_x_raw)\n",
    "df_y = pd.DataFrame(train_y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c05a8884dfda7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Add all of the preprocessing below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39710aab3026879d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 1 - 146 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d335519eab176102",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:05:09.130687Z",
     "start_time": "2024-03-27T18:05:09.055773Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['c_abrv', 'f46_IT', 'v72_DE', 'v73_DE', 'v74_DE', 'v75_DE', 'v76_DE', 'v77_DE', 'v78_DE', 'v79_DE']\n",
    "df_train.drop(columns=columns_to_drop, inplace=True)\n",
    "df_test.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef956e35e6f124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 147 - 292 Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f908a39187ffa8bf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:05:09.146791Z",
     "start_time": "2024-03-27T18:05:09.131708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v30c', 'v45c', 'v133_11c', 'v134_11c', 'v135_11c', 'v136_11c', 'v137_11c', 'v138_11c', 'v139_11c', 'v140_11c', 'v141_11c']\n",
      "['age_r', 'v228b_r', 'v231b_r', 'v233b_r', 'v239_r', 'v242_r', 'v243_r', 'v251b_r', 'v252_r', 'v261_r', 'v262_r', 'v263_r', 'v276_r', 'v278c_r', 'v279c_r', 'v279d_r', 'v281a_r']\n"
     ]
    }
   ],
   "source": [
    "### Function to find the targeted colname\n",
    "def find_colname(data, target):\n",
    "    temp = []\n",
    "    for varname in data.columns:\n",
    "        if varname.endswith(target):\n",
    "            temp.append(varname)\n",
    "    return(temp)\n",
    "\n",
    "merge_colname = find_colname(train_x_raw, '_11c')\n",
    "print(find_colname(train_x_raw, 'c'))\n",
    "print(find_colname(train_x_raw, '_r'))\n",
    "\n",
    "def merge_columns(dat, colname):\n",
    "    for name in colname:\n",
    "        name_org = name.replace(\"_11c\", \"\")\n",
    "        dat.loc[dat[name_org] == -4, name_org] = dat.loc[dat[name_org] == -4, name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d251374774258cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Variable 293 - 438 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74a294a5eb2c211d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:05:09.757547Z",
     "start_time": "2024-03-27T18:05:09.149891Z"
    }
   },
   "outputs": [],
   "source": [
    "## removed string type data\n",
    "df_train.drop('v228b', inplace=True, axis=1) \n",
    "df_test.drop('v228b', inplace=True, axis=1) \n",
    "\n",
    "df_train.fillna({'v228b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v228b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v231b', inplace=True, axis=1) \n",
    "df_test.drop('v231b', inplace=True, axis=1)\n",
    "\n",
    "df_train.fillna({'v231b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v231b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v233b', inplace=True, axis=1)\n",
    "df_test.drop('v233b', inplace=True, axis=1)\n",
    "\n",
    "df_train.fillna({'v233b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v233b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('v251b', inplace=True, axis=1)\n",
    "df_test.drop('v251b', inplace=True, axis=1) \n",
    "\n",
    "df_train.fillna({'v251b_r': -3}, inplace=True)\n",
    "df_test.fillna({'v251b_r': -3}, inplace=True)\n",
    "\n",
    "df_train.drop('f252_edulvlb_CH', inplace=True, axis=1)\n",
    "df_test.drop('f252_edulvlb_CH', inplace=True, axis=1)\n",
    "\n",
    "## removed the column having 'DE'\n",
    "df_train.drop(list(df_train.filter(regex='DE')), axis=1, inplace=True)\n",
    "df_test.drop(list(df_test.filter(regex='DE')), axis=1, inplace=True)\n",
    "\n",
    "## removed the column having 'GB'\n",
    "df_train.drop(list(df_train.filter(regex='GB')), axis=1, inplace=True)\n",
    "df_test.drop(list(df_test.filter(regex='GB')), axis=1, inplace=True)\n",
    "\n",
    "df_train.drop('v281a', inplace=True, axis=1)\n",
    "df_test.drop('v281a', inplace=True, axis=1)\n",
    "\n",
    "df_train.drop('v275b_N2', inplace=True, axis=1) \n",
    "df_test.drop('v275b_N2', inplace=True, axis=1) \n",
    "\n",
    "df_train.drop('v275b_N1', inplace=True, axis=1) \n",
    "df_test.drop('v275b_N1', inplace=True, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646daeb6719b84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Model Run\n",
    "##  xgBoost model set up"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "label_mapping = {-1: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "df_y = df_y.replace(label_mapping)\n",
    "\n",
    "dtrain = xgb.DMatrix(df_train, label=df_y, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(df_test, enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 5,\n",
    "    'eval_metric': 'mlogloss',\n",
    "}\n",
    "\n",
    "num_boost_round = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:05:10.537531Z",
     "start_time": "2024-03-27T18:05:09.758625Z"
    }
   },
   "id": "5fbfcd048b55d241",
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73377ae83e283639"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    train-mlogloss-mean  train-mlogloss-std  test-mlogloss-mean  \\\n0              1.600916            0.000046            1.601236   \n1              1.592535            0.000074            1.593170   \n2              1.584288            0.000099            1.585240   \n3              1.576180            0.000126            1.577438   \n4              1.568210            0.000149            1.569779   \n5              1.560376            0.000181            1.562257   \n6              1.552658            0.000202            1.554847   \n7              1.545067            0.000226            1.547564   \n8              1.537589            0.000247            1.540389   \n9              1.530241            0.000275            1.533344   \n10             1.522987            0.000298            1.526398   \n11             1.515849            0.000312            1.519553   \n12             1.508816            0.000329            1.512825   \n13             1.501889            0.000354            1.506207   \n14             1.495065            0.000371            1.499684   \n15             1.488340            0.000382            1.493254   \n16             1.481707            0.000405            1.486935   \n17             1.475171            0.000415            1.480690   \n18             1.468740            0.000434            1.474578   \n19             1.462382            0.000449            1.468499   \n20             1.456120            0.000461            1.462531   \n21             1.449931            0.000473            1.456635   \n22             1.443830            0.000483            1.450826   \n23             1.437817            0.000500            1.445089   \n24             1.431877            0.000514            1.439426   \n25             1.426028            0.000515            1.433863   \n26             1.420260            0.000535            1.428364   \n27             1.414560            0.000552            1.422942   \n28             1.408928            0.000552            1.417588   \n29             1.403379            0.000567            1.412317   \n30             1.397898            0.000577            1.407113   \n31             1.392476            0.000581            1.401976   \n32             1.387144            0.000600            1.396904   \n33             1.381867            0.000615            1.391907   \n34             1.376659            0.000628            1.386992   \n35             1.371520            0.000645            1.382136   \n36             1.366438            0.000656            1.377330   \n37             1.361431            0.000687            1.372584   \n38             1.356471            0.000701            1.367904   \n39             1.351571            0.000717            1.363274   \n40             1.346719            0.000735            1.358693   \n41             1.341943            0.000757            1.354197   \n42             1.337205            0.000760            1.349729   \n43             1.332544            0.000763            1.345338   \n44             1.327905            0.000775            1.340965   \n45             1.323347            0.000778            1.336687   \n46             1.318830            0.000800            1.332436   \n47             1.314368            0.000805            1.328234   \n48             1.309953            0.000821            1.324092   \n49             1.305590            0.000821            1.319997   \n50             1.301260            0.000825            1.315930   \n51             1.296996            0.000837            1.311946   \n52             1.292770            0.000836            1.307989   \n53             1.288583            0.000836            1.304060   \n54             1.284454            0.000836            1.300193   \n55             1.280363            0.000835            1.296361   \n56             1.276316            0.000831            1.292574   \n57             1.272300            0.000833            1.288816   \n58             1.268356            0.000835            1.285138   \n59             1.264434            0.000829            1.281469   \n60             1.260569            0.000839            1.277861   \n61             1.256745            0.000836            1.274285   \n62             1.252952            0.000840            1.270748   \n63             1.249193            0.000850            1.267252   \n64             1.245471            0.000855            1.263787   \n65             1.241798            0.000875            1.260376   \n66             1.238169            0.000883            1.257001   \n67             1.234564            0.000897            1.253660   \n68             1.231006            0.000907            1.250355   \n69             1.227507            0.000910            1.247105   \n70             1.224018            0.000927            1.243880   \n71             1.220564            0.000918            1.240673   \n72             1.217161            0.000937            1.237536   \n73             1.213777            0.000943            1.234403   \n74             1.210440            0.000949            1.231314   \n75             1.207127            0.000971            1.228260   \n76             1.203848            0.000976            1.225231   \n77             1.200604            0.000992            1.222234   \n78             1.197385            0.001008            1.219272   \n79             1.194197            0.001011            1.216346   \n80             1.191034            0.001002            1.213443   \n81             1.187889            0.001000            1.210551   \n82             1.184804            0.000995            1.207711   \n83             1.181747            0.000999            1.204903   \n84             1.178710            0.001005            1.202130   \n85             1.175706            0.001004            1.199368   \n86             1.172732            0.001002            1.196645   \n87             1.169786            0.000996            1.193951   \n88             1.166860            0.000998            1.191273   \n89             1.163979            0.001003            1.188629   \n90             1.161103            0.000989            1.186005   \n91             1.158273            0.000997            1.183419   \n92             1.155462            0.000994            1.180860   \n93             1.152668            0.000989            1.178312   \n94             1.149932            0.001000            1.175816   \n95             1.147201            0.001006            1.173336   \n96             1.144499            0.001008            1.170890   \n97             1.141822            0.001016            1.168458   \n98             1.139165            0.001007            1.166051   \n99             1.136541            0.001012            1.163683   \n\n    test-mlogloss-std  \n0            0.000044  \n1            0.000085  \n2            0.000130  \n3            0.000171  \n4            0.000216  \n5            0.000239  \n6            0.000284  \n7            0.000320  \n8            0.000350  \n9            0.000392  \n10           0.000412  \n11           0.000465  \n12           0.000489  \n13           0.000517  \n14           0.000542  \n15           0.000586  \n16           0.000605  \n17           0.000638  \n18           0.000644  \n19           0.000678  \n20           0.000700  \n21           0.000739  \n22           0.000773  \n23           0.000794  \n24           0.000839  \n25           0.000878  \n26           0.000920  \n27           0.000941  \n28           0.000971  \n29           0.001005  \n30           0.001030  \n31           0.001066  \n32           0.001095  \n33           0.001113  \n34           0.001137  \n35           0.001154  \n36           0.001189  \n37           0.001188  \n38           0.001215  \n39           0.001233  \n40           0.001248  \n41           0.001241  \n42           0.001274  \n43           0.001303  \n44           0.001309  \n45           0.001334  \n46           0.001332  \n47           0.001354  \n48           0.001362  \n49           0.001389  \n50           0.001422  \n51           0.001420  \n52           0.001461  \n53           0.001485  \n54           0.001512  \n55           0.001539  \n56           0.001566  \n57           0.001578  \n58           0.001603  \n59           0.001646  \n60           0.001659  \n61           0.001695  \n62           0.001726  \n63           0.001746  \n64           0.001759  \n65           0.001764  \n66           0.001769  \n67           0.001784  \n68           0.001785  \n69           0.001808  \n70           0.001819  \n71           0.001854  \n72           0.001862  \n73           0.001881  \n74           0.001890  \n75           0.001895  \n76           0.001903  \n77           0.001905  \n78           0.001904  \n79           0.001922  \n80           0.001935  \n81           0.001942  \n82           0.001964  \n83           0.001986  \n84           0.002005  \n85           0.002015  \n86           0.002035  \n87           0.002030  \n88           0.002058  \n89           0.002069  \n90           0.002080  \n91           0.002094  \n92           0.002110  \n93           0.002120  \n94           0.002131  \n95           0.002132  \n96           0.002147  \n97           0.002151  \n98           0.002170  \n99           0.002184  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train-mlogloss-mean</th>\n      <th>train-mlogloss-std</th>\n      <th>test-mlogloss-mean</th>\n      <th>test-mlogloss-std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.600916</td>\n      <td>0.000046</td>\n      <td>1.601236</td>\n      <td>0.000044</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.592535</td>\n      <td>0.000074</td>\n      <td>1.593170</td>\n      <td>0.000085</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.584288</td>\n      <td>0.000099</td>\n      <td>1.585240</td>\n      <td>0.000130</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.576180</td>\n      <td>0.000126</td>\n      <td>1.577438</td>\n      <td>0.000171</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.568210</td>\n      <td>0.000149</td>\n      <td>1.569779</td>\n      <td>0.000216</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.560376</td>\n      <td>0.000181</td>\n      <td>1.562257</td>\n      <td>0.000239</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.552658</td>\n      <td>0.000202</td>\n      <td>1.554847</td>\n      <td>0.000284</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.545067</td>\n      <td>0.000226</td>\n      <td>1.547564</td>\n      <td>0.000320</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.537589</td>\n      <td>0.000247</td>\n      <td>1.540389</td>\n      <td>0.000350</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.530241</td>\n      <td>0.000275</td>\n      <td>1.533344</td>\n      <td>0.000392</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.522987</td>\n      <td>0.000298</td>\n      <td>1.526398</td>\n      <td>0.000412</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.515849</td>\n      <td>0.000312</td>\n      <td>1.519553</td>\n      <td>0.000465</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.508816</td>\n      <td>0.000329</td>\n      <td>1.512825</td>\n      <td>0.000489</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.501889</td>\n      <td>0.000354</td>\n      <td>1.506207</td>\n      <td>0.000517</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.495065</td>\n      <td>0.000371</td>\n      <td>1.499684</td>\n      <td>0.000542</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.488340</td>\n      <td>0.000382</td>\n      <td>1.493254</td>\n      <td>0.000586</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1.481707</td>\n      <td>0.000405</td>\n      <td>1.486935</td>\n      <td>0.000605</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1.475171</td>\n      <td>0.000415</td>\n      <td>1.480690</td>\n      <td>0.000638</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.468740</td>\n      <td>0.000434</td>\n      <td>1.474578</td>\n      <td>0.000644</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.462382</td>\n      <td>0.000449</td>\n      <td>1.468499</td>\n      <td>0.000678</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.456120</td>\n      <td>0.000461</td>\n      <td>1.462531</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1.449931</td>\n      <td>0.000473</td>\n      <td>1.456635</td>\n      <td>0.000739</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1.443830</td>\n      <td>0.000483</td>\n      <td>1.450826</td>\n      <td>0.000773</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.437817</td>\n      <td>0.000500</td>\n      <td>1.445089</td>\n      <td>0.000794</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.431877</td>\n      <td>0.000514</td>\n      <td>1.439426</td>\n      <td>0.000839</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.426028</td>\n      <td>0.000515</td>\n      <td>1.433863</td>\n      <td>0.000878</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.420260</td>\n      <td>0.000535</td>\n      <td>1.428364</td>\n      <td>0.000920</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.414560</td>\n      <td>0.000552</td>\n      <td>1.422942</td>\n      <td>0.000941</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.408928</td>\n      <td>0.000552</td>\n      <td>1.417588</td>\n      <td>0.000971</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.403379</td>\n      <td>0.000567</td>\n      <td>1.412317</td>\n      <td>0.001005</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1.397898</td>\n      <td>0.000577</td>\n      <td>1.407113</td>\n      <td>0.001030</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>1.392476</td>\n      <td>0.000581</td>\n      <td>1.401976</td>\n      <td>0.001066</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1.387144</td>\n      <td>0.000600</td>\n      <td>1.396904</td>\n      <td>0.001095</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>1.381867</td>\n      <td>0.000615</td>\n      <td>1.391907</td>\n      <td>0.001113</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>1.376659</td>\n      <td>0.000628</td>\n      <td>1.386992</td>\n      <td>0.001137</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1.371520</td>\n      <td>0.000645</td>\n      <td>1.382136</td>\n      <td>0.001154</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1.366438</td>\n      <td>0.000656</td>\n      <td>1.377330</td>\n      <td>0.001189</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>1.361431</td>\n      <td>0.000687</td>\n      <td>1.372584</td>\n      <td>0.001188</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>1.356471</td>\n      <td>0.000701</td>\n      <td>1.367904</td>\n      <td>0.001215</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>1.351571</td>\n      <td>0.000717</td>\n      <td>1.363274</td>\n      <td>0.001233</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1.346719</td>\n      <td>0.000735</td>\n      <td>1.358693</td>\n      <td>0.001248</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1.341943</td>\n      <td>0.000757</td>\n      <td>1.354197</td>\n      <td>0.001241</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1.337205</td>\n      <td>0.000760</td>\n      <td>1.349729</td>\n      <td>0.001274</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>1.332544</td>\n      <td>0.000763</td>\n      <td>1.345338</td>\n      <td>0.001303</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>1.327905</td>\n      <td>0.000775</td>\n      <td>1.340965</td>\n      <td>0.001309</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1.323347</td>\n      <td>0.000778</td>\n      <td>1.336687</td>\n      <td>0.001334</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>1.318830</td>\n      <td>0.000800</td>\n      <td>1.332436</td>\n      <td>0.001332</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>1.314368</td>\n      <td>0.000805</td>\n      <td>1.328234</td>\n      <td>0.001354</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>1.309953</td>\n      <td>0.000821</td>\n      <td>1.324092</td>\n      <td>0.001362</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>1.305590</td>\n      <td>0.000821</td>\n      <td>1.319997</td>\n      <td>0.001389</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>1.301260</td>\n      <td>0.000825</td>\n      <td>1.315930</td>\n      <td>0.001422</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>1.296996</td>\n      <td>0.000837</td>\n      <td>1.311946</td>\n      <td>0.001420</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>1.292770</td>\n      <td>0.000836</td>\n      <td>1.307989</td>\n      <td>0.001461</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>1.288583</td>\n      <td>0.000836</td>\n      <td>1.304060</td>\n      <td>0.001485</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>1.284454</td>\n      <td>0.000836</td>\n      <td>1.300193</td>\n      <td>0.001512</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>1.280363</td>\n      <td>0.000835</td>\n      <td>1.296361</td>\n      <td>0.001539</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>1.276316</td>\n      <td>0.000831</td>\n      <td>1.292574</td>\n      <td>0.001566</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>1.272300</td>\n      <td>0.000833</td>\n      <td>1.288816</td>\n      <td>0.001578</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>1.268356</td>\n      <td>0.000835</td>\n      <td>1.285138</td>\n      <td>0.001603</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>1.264434</td>\n      <td>0.000829</td>\n      <td>1.281469</td>\n      <td>0.001646</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>1.260569</td>\n      <td>0.000839</td>\n      <td>1.277861</td>\n      <td>0.001659</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>1.256745</td>\n      <td>0.000836</td>\n      <td>1.274285</td>\n      <td>0.001695</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>1.252952</td>\n      <td>0.000840</td>\n      <td>1.270748</td>\n      <td>0.001726</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>1.249193</td>\n      <td>0.000850</td>\n      <td>1.267252</td>\n      <td>0.001746</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>1.245471</td>\n      <td>0.000855</td>\n      <td>1.263787</td>\n      <td>0.001759</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>1.241798</td>\n      <td>0.000875</td>\n      <td>1.260376</td>\n      <td>0.001764</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>1.238169</td>\n      <td>0.000883</td>\n      <td>1.257001</td>\n      <td>0.001769</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>1.234564</td>\n      <td>0.000897</td>\n      <td>1.253660</td>\n      <td>0.001784</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>1.231006</td>\n      <td>0.000907</td>\n      <td>1.250355</td>\n      <td>0.001785</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>1.227507</td>\n      <td>0.000910</td>\n      <td>1.247105</td>\n      <td>0.001808</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>1.224018</td>\n      <td>0.000927</td>\n      <td>1.243880</td>\n      <td>0.001819</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>1.220564</td>\n      <td>0.000918</td>\n      <td>1.240673</td>\n      <td>0.001854</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>1.217161</td>\n      <td>0.000937</td>\n      <td>1.237536</td>\n      <td>0.001862</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>1.213777</td>\n      <td>0.000943</td>\n      <td>1.234403</td>\n      <td>0.001881</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>1.210440</td>\n      <td>0.000949</td>\n      <td>1.231314</td>\n      <td>0.001890</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>1.207127</td>\n      <td>0.000971</td>\n      <td>1.228260</td>\n      <td>0.001895</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>1.203848</td>\n      <td>0.000976</td>\n      <td>1.225231</td>\n      <td>0.001903</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1.200604</td>\n      <td>0.000992</td>\n      <td>1.222234</td>\n      <td>0.001905</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>1.197385</td>\n      <td>0.001008</td>\n      <td>1.219272</td>\n      <td>0.001904</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>1.194197</td>\n      <td>0.001011</td>\n      <td>1.216346</td>\n      <td>0.001922</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>1.191034</td>\n      <td>0.001002</td>\n      <td>1.213443</td>\n      <td>0.001935</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>1.187889</td>\n      <td>0.001000</td>\n      <td>1.210551</td>\n      <td>0.001942</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>1.184804</td>\n      <td>0.000995</td>\n      <td>1.207711</td>\n      <td>0.001964</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>1.181747</td>\n      <td>0.000999</td>\n      <td>1.204903</td>\n      <td>0.001986</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>1.178710</td>\n      <td>0.001005</td>\n      <td>1.202130</td>\n      <td>0.002005</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>1.175706</td>\n      <td>0.001004</td>\n      <td>1.199368</td>\n      <td>0.002015</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>1.172732</td>\n      <td>0.001002</td>\n      <td>1.196645</td>\n      <td>0.002035</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>1.169786</td>\n      <td>0.000996</td>\n      <td>1.193951</td>\n      <td>0.002030</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>1.166860</td>\n      <td>0.000998</td>\n      <td>1.191273</td>\n      <td>0.002058</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>1.163979</td>\n      <td>0.001003</td>\n      <td>1.188629</td>\n      <td>0.002069</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>1.161103</td>\n      <td>0.000989</td>\n      <td>1.186005</td>\n      <td>0.002080</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>1.158273</td>\n      <td>0.000997</td>\n      <td>1.183419</td>\n      <td>0.002094</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>1.155462</td>\n      <td>0.000994</td>\n      <td>1.180860</td>\n      <td>0.002110</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>1.152668</td>\n      <td>0.000989</td>\n      <td>1.178312</td>\n      <td>0.002120</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>1.149932</td>\n      <td>0.001000</td>\n      <td>1.175816</td>\n      <td>0.002131</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>1.147201</td>\n      <td>0.001006</td>\n      <td>1.173336</td>\n      <td>0.002132</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>1.144499</td>\n      <td>0.001008</td>\n      <td>1.170890</td>\n      <td>0.002147</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>1.141822</td>\n      <td>0.001016</td>\n      <td>1.168458</td>\n      <td>0.002151</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>1.139165</td>\n      <td>0.001007</td>\n      <td>1.166051</td>\n      <td>0.002170</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>1.136541</td>\n      <td>0.001012</td>\n      <td>1.163683</td>\n      <td>0.002184</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "# params and num_boost_round provided above\n",
    "xgb_cv = cv(dtrain=dtrain, params=params, nfold=5,\n",
    "            num_boost_round=num_boost_round, early_stopping_rounds=10,\n",
    "            metrics=\"mlogloss\", as_pandas=True, seed=123)\n",
    "\n",
    "xgb_cv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:06:42.022458Z",
     "start_time": "2024-03-27T18:05:10.540713Z"
    }
   },
   "id": "ee6d6fdecf41fb2a",
   "execution_count": 107
  },
  {
   "cell_type": "markdown",
   "source": [
    "## xgboost train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf3eff0c1cff4b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Multiclass Logarithmic Loss: 1.1411852520567676\n",
      "Validation Multiclass Logarithmic Loss: 1.1411852418314379\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}\n",
    "bst = xgb.train(params, dtrain, num_boost_round, \n",
    "                evals=[(dtrain, 'train')], evals_result=evals_result, \n",
    "                verbose_eval=False)\n",
    "print(f\"Training Multiclass Logarithmic Loss: {evals_result['train']['mlogloss'][-1]}\")\n",
    "\n",
    "y_test_probs = bst.predict(dtest)\n",
    "\n",
    "class_order = [0, 1, 2, 3, 4]\n",
    "class_mapping = {class_label: f\"Class_{class_label}\" for class_label in class_order}\n",
    "\n",
    "y_train_probs = bst.predict(dtrain)\n",
    "val_log_loss = log_loss(df_y, y_train_probs, labels=class_order)\n",
    "print(f\"Validation Multiclass Logarithmic Loss: {val_log_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:07:06.878598Z",
     "start_time": "2024-03-27T18:06:42.025685Z"
    }
   },
   "id": "7e33513991df267c",
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Submission csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1527521c5801211"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(y_test_probs, columns=class_mapping.values())\n",
    "submission_df.columns = ['no answer', 'very important', 'quite important', 'not important', 'not at all important']\n",
    "submission_df.insert(0, 'id', df_test.index)\n",
    "\n",
    "# Save the submission file\n",
    "# submission_file = ('submission.csv')\n",
    "# submission_df.to_csv(submission_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T18:07:06.910218Z",
     "start_time": "2024-03-27T18:07:06.882783Z"
    }
   },
   "id": "5ae6668498e2687f",
   "execution_count": 109
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
